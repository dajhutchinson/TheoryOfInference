\documentclass[11pt,a4paper]{article}

\usepackage[margin=1in, paperwidth=8.3in, paperheight=11.7in]{geometry}
\usepackage[ruled, vlined, linesnumbered]{algorithm2e}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{dsfont}
\usepackage{enumerate}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{changepage} 

\begin{document}

\pagestyle{fancy}
\setlength\parindent{0pt}
\allowdisplaybreaks

\renewcommand{\headrulewidth}{0pt}
\setlist[enumerate,1]{label={\roman*)}}

% Cover page title
\title{Theory of Inference - Notes}
\author{Dom Hutchinson}
\date{\today}
\maketitle

% Header
\fancyhead[L]{Dom Hutchinson}
\fancyhead[C]{Theory of Inference - Notes}
\fancyhead[R]{\today}

% Counters
\newcounter{definition}[section]
\newcounter{example}[section]
\newcounter{notation}[section]
\newcounter{proposition}[section]
\newcounter{proof}[section]
\newcounter{remark}[section]
\newcounter{theorem}[section]

% commands
\newcommand{\dotprod}[0]{\boldsymbol{\cdot}}
\newcommand{\cosech}[0]{\mathrm{cosech}\ }
\newcommand{\cosec}[0]{\mathrm{cosec}\ }
\newcommand{\sech}[0]{\mathrm{sech}\ }
\newcommand{\prob}[0]{\mathbb{P}}
\newcommand{\nats}[0]{\mathbb{N}}
\newcommand{\cov}[0]{\mathrm{Cov}}
\newcommand{\var}[0]{\mathrm{Var}}
\newcommand{\expect}[0]{\mathbb{E}}
\newcommand{\reals}[0]{\mathbb{R}}
\newcommand{\integers}[0]{\mathbb{Z}}
\newcommand{\indicator}[0]{\mathds{1}}
\newcommand{\nb}[0]{\textit{N.B.} }
\newcommand{\ie}[0]{\textit{i.e.} }
\newcommand{\eg}[0]{\textit{e.g.} }
\newcommand{\X}[0]{\textbf{X}}
\newcommand{\x}[0]{\textbf{x}}
\newcommand{\iid}[0]{\overset{\text{iid}}{\sim}}
\newcommand{\proved}[0]{$\hfill\square$\\}

\newcommand{\definition}[1]{\stepcounter{definition} \textbf{Definition \arabic{section}.\arabic{definition}\ - }\textit{#1}\\}
\newcommand{\definitionn}[1]{\stepcounter{definition} \textbf{Definition \arabic{section}.\arabic{definition}\ - }\textit{#1}}
\newcommand{\proof}[1]{\stepcounter{proof} \textbf{Proof \arabic{section}.\arabic{proof}\ - }\textit{#1}\\}
\newcommand{\prooff}[1]{\stepcounter{proof} \textbf{Proof \arabic{section}.\arabic{proof}\ - }\textit{#1}}
\newcommand{\example}[1]{\stepcounter{example} \textbf{Example \arabic{section}.\arabic{example}\ - }\textit{#1}\\}
\newcommand{\examplee}[1]{\stepcounter{example} \textbf{Example \arabic{section}.\arabic{example}\ - }\textit{#1}}
\newcommand{\notation}[1]{\stepcounter{notation} \textbf{Notation \arabic{section}.\arabic{notation}\ - }\textit{#1}\\}
\newcommand{\notationn}[1]{\stepcounter{notation} \textbf{Notation \arabic{section}.\arabic{notation}\ - }\textit{#1}}
\newcommand{\proposition}[1]{\stepcounter{proposition} \textbf{Proposition \arabic{section}.\arabic{proposition}\ - }\textit{#1}\\}
\newcommand{\propositionn}[1]{\stepcounter{proposition} \textbf{Proposition \arabic{section}.\arabic{proposition}\ - }\textit{#1}}
\newcommand{\remark}[1]{\stepcounter{remark} \textbf{Remark \arabic{section}.\arabic{remark}\ - }\textit{#1}\\}
\newcommand{\remarkk}[1]{\stepcounter{remark} \textbf{Remark \arabic{section}.\arabic{remark}\ - }\textit{#1}}
\newcommand{\theorem}[1]{\stepcounter{theorem} \textbf{Theorem \arabic{section}.\arabic{theorem}\ - }\textit{#1}\\}
\newcommand{\theoremm}[1]{\stepcounter{theorem} \textbf{Theorem \arabic{section}.\arabic{theorem}\ - }\textit{#1}}

\tableofcontents

% Start of content
\newpage

\section{Motivation}

\remark{General Idea}
Learn something about the world using data \& statistical models.\\

\definition{Statistical Models}
\textit{Statistical Models} describe the way in which data is generate. They depend upon \textit{unknown} constant parameters, $\pmb\theta$, and subsidiary information (known data \& parameters).\\

\definition{Parameteric Statistical Inference}
\textit{Parameteric Statistical Inference} is the process of taking some data \& learning the \textit{unknown} parameters of the model which generated it.

\definition{Parameteric Models}
A \textit{Parameteric Model} is a statistical model whose pdf depends on some unknown parameter.\\
A \textit{Semi-Parameteric Models} is a statistical models which contains unknown functions, as well as unknown parameters.\\
A \textit{Non-Parameteric Model} has no parameters and thus makes minimal assumptions about how the data was generated.\\

\proposition{Inferential Questions}
When performing \textit{Statistical Inference} we wish to answer the following questions
\begin{enumerate}
	\item \textit{Confidence Intervals \& Credible Intervals} - What range of parameter valeus are consistent with the data?
	\item \textit{Hypothesis Testing} - Are some pre-specified valeus (or restrictions) for the parameters consistent with the data?
	\item \textit{Model Checking} - Could our model have generated the data at all?
	\item \textit{Model Selection} - Which of several alternative odels could most plausibly have generated the data?
	\item \textit{Statistical Design} - How could we better arrange teh data gathering process to improve the answers to the preceding questions?
\end{enumerate}

\subsection{Examples}

\example{Mean Annual Temperatures}
Consider a dataset of the mean annual temperature in New Haven, Conneticut.\\
Suppose we plot it in a histogram \& notice that it fits a bell curve, then we may assume the data fits a simple model where each data point is observed independently from a $\mathcal{N}(\mu,\sigma^2)$ distribution with $\mu,\sigma^2$ unknown.\\
Then the pdf for each data point, $y_i$, is
$$f(y_i)=\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac1{2\sigma^2}(y_i-\mu)^2}$$
The pdf for the whole data set, $\textbf{y}$, is the joint pdf of each data point since we assume iid
$$f(\textbf{y})=\prod_{i=1}^Nf(y_i)$$
Now suppose we notice that the histogram is \textit{heavy tailed} relative to a normal distribution.\\
A better model might be
$$\frac{y_i-\mu}\sigma\sim t_\alpha$$
where $\mu,\sigma^2,\alpha$ are unknown.\\
This means the pdf of the whole data set is
$$f(\textbf{y})=\prod_{i=1}^N\frac{1}{\sigma}f_{t_\alpha}\left(\frac{y_i-\mu}{\sigma}\right)$$
by \textit{standard transformation theory}.\\

\example{Hourly Air Temperature}
Consider a dataset of the air temperature, $a_i$, measured at hourly intervals, $t_i$, over the course of a week.\\
The temperature is believed to follow a daily cycle, with a long-term dift over the course of the week and to be subject to random autocorrelated depatures from this overall pattern.\\
A suitable model might be
$$a_i=\underbrace{\theta_0+\theta_1t_i}_\text{Long-Term Drift}+\underbrace{\theta_2\sin(2\pi t_i/24)+\theta_3\cos(2\pi t_i/24)}_\text{Daily Cycle}+\underbrace{e_i}_\text{Auto Correlation}$$
where $e_{i+1}:=\rho r_i+\varepsilon_i$ with $|\rho|<1$ \& $\varepsilon\iid\mathcal{N}(0,\sigma^2)$.\\
This means $\textbf{e}\sim\mathcal{N}(\pmb0,\Sigma)$ \& $\textbf{a}\sim\mathcal{N}(\pmb\mu,\Sigma)$ with $\Sigma_{i,j}=\frac{\rho^{|1-j|}\sigma^2}{1-\rho}$.\\
Thus, the pdf of the data set, $\textbf{a}$, is
$$f(\textbf{a})=\frac{1}{\sqrt{(2\pi)^n|\Sigma|}}e^{-\frac{1}{2}(\textbf{a}-\pmb\mu)^T\Sigma^{-1}(\textbf{a}-\pmb\mu)}$$

\example{Bone Marrow}
Consider a dataset produced 23 patients suffering from non-Hodgkin's Lymphoma are split into two groups, each recieving a different treatment. We wish to test whether one of these treatments is more efficitive than the other.\\
For each patient the days between treatment \& relapse was recorded. We have some \textit{censored data} as the patient had not relapsed by the time of their last appointment.\\
Consider using an exponential distribution to model the times to relapse with parameters $\theta_a$ \& $\theta_b$ respecitvely. We want to test if $\theta_a=\theta_b$.\\
We have the follow pdf for patients in group $a$
$$f_a(t_i)=\begin{cases}\theta_ae^{-\theta_at_i}&\text{uncensored}\\\int_{t_i}^\infty\theta_ae^{-\theta_at_i}=e^{-\theta_at_i}&\text{censored}\end{cases}$$
An equivalent pdf exists for patients in group $b$, with $\theta_b$ swapped in.\\
Thus the model for the whole data set, $\textbf{t}$, is
$$f(\textbf{t})=\prod_{i=1}^{11}f_a(t_i)\prod_{i=12}^{23}f_b(t_i)$$
when patients $\{1,\dots,11\}$ are in group $a$ and the rest in group $b$.

\section{Basic Approaches to Inference}

\definition{Frequentist Approach}
In the \textit{Frequentist Approach} to inference we assume the model parameters are fixed states, which we wish to estimate. The parameter estimator $\hat\theta$ is a random variable which inherits its randomnewss from the data which it is constructed from.\\

\definition{Bayesian Approach}
In the \textit{Bayesian Approach} to inference model parameters are treated as random variables and we use probability distributions to encode our uncertainty about the parameters. We set a prior distribution, $\prob(\theta)$, and then use data to update it and learn a posterior distribution, $\prob(\theta|\x)$.\\

\remark{Assumptions}
Often we are required to make assumptions in order to analyse the results these approaches. For the \textit{Frequentist Approach} we often assume we have a large data set, whilst for the \textit{Bayesian Approach} we produce simulations from the posterior.\\

\example{Comparing Frequentist \& Bayesian Approach}
Let $X_1,\dots,X_n\iid\text{Normal}(\mu,1)$ where $\mu$ is an unknown parameter we wish to learn.\\
Let $\x:=\{x_1,\dots,x_n\}$  be a realisation of $\X$.
\begin{itemize}
	\item[Frequentist]Let's use $\hat\mu=\bar{x}=\frac{1}{n}\sum_{i=1}^n x_i$.\\
	Consider the expectation and variance of $\hat\mu$
	$$\expect(\hat\mu)=\expect(\bar{x})=\frac{1}{n}\sum_{i=1}^n\expect(X_i)=\mu\text{ and }\var(\hat\mu)=\var(\bar{x})=\frac{1}{n^2}\sum_{i=1}^n\var(X_i)=\frac{1}{n}$$
	Since $\hat\mu$ is a linear transformation of normal random variables it has a normal random variable, thus
	$$\hat\mu\sim\text{Normal}\left(\mu,\frac1n\right)$$
	Thus $\hat\mu$ is an \textit{unbiased} estimator of $\mu$.\\
	By noting that $\sqrt{n}(\hat\mu-\mu)\sim\text{Normal}(0,1)$ we can construct \textit{Confidence Intervals} for $\mu$
	\[\begin{array}{rcl}
	0.95&=&\prob(-1.96<\sqrt{n}(\hat\mu-\mu)<1.96)\\
	\implies0.95&=&\prob\left(\hat\mu-\dfrac{1.96}{\sqrt{n}}<\mu<\hat\mu+\dfrac{1.96}{\sqrt{n}}\right)
	\end{array}\]
	\item[Bayesian] Here we treat $\mu$ as a random variable and thus must choose a distribution for it
	$$\mu\sim\text{Normal}(0,\sigma^2_\mu)$$
	where $\sigma^2_\mu$ is a value we set. Generally we choose greater values for the variance when we are less certain.\\
	We want to find $\prob(\mu|\x)$ and note that \textit{Bayes' Rule} states
	$$\prob(\mu|\x)=\dfrac{\prob(\x|\mu)\prob(\mu)}{\prob(\x)}$$
	In this setting $\prob(\x)$ is intractable so we use a trick that since $\prob(\x)$ is a normalising factor we have
	$$\prob(\mu|\x)\propto\prob(\x|\mu)\prob(\mu)$$
	From this proportionality we aim to identity the distribution of $\prob(\mu|\x)$.
	\[\begin{array}{rcl}
	\prob(\mu|\x)&\propto&\text{exp}\left\{-\dfrac{1}{2\sigma^2_\mu}\displaystyle\sum_{i=1}^n[(x_i-\mu)^2+\mu^2]\right\}\\
	&\propto&\text{exp}\left\{-\dfrac{1}{2}\left(-2n\bar{x}\mu+\dfrac{\mu^2(n\sigma^2_\mu+1)}{\sigma^2_\mu}\right)\right\}\\
	&\propto&\text{exp}\left\{-\dfrac12\left(\dfrac{n\sigma^2_\mu+1}{\sigma^2_\mu}\right)\left(\mu^2-2\bar{x}\mu\dfrac{n\sigma^2_\mu}{n\sigma^2_\mu+1}\right)\right\}\\
	&\propto&\text{exp}\Bigg\{-\dfrac12\underbrace{\left(\dfrac{n\sigma^2_\mu+1}{\sigma^2_\mu}\right)}_{1/\sigma^2}\underbrace{\left(\mu-\bar{x}\dfrac{n\sigma^2_\mu}{n\sigma^2_\mu+1}\right)^2}_\mu\Bigg\}\text{ by completing the square}
	\end{array}\]
	We can produce a \textit{Credible Interval} for $\mu$ as
	$$\bar{x}\dfrac{n\sigma^2_\mu}{n\sigma^2_\mu+1}\pm1.96\dfrac{\sigma_mu}{\sqrt{n\sigma^2_\mu+1}}$$
\end{itemize}
If we consider the final distribution from the \textit{Bayesian Approach} as $n\to\infty$ we notice that
$$\mu|\x\to\bar{x}=\hat\mu\quad\text{and}\quad\sigma^2|\x\to\frac1n$$

\subsection{Inference by Resampling}

\remark{Motivation}
The uncertainty we have about a parameter is inherited from the uncertainty in the data sampling process. Often we have a data set \& are unable to repeat the data gathering process, and even if we could we would just combine it into a larger sample rather than split it.\\

\definition{Resampling}
Let $\x$ be a given data set.\\
We can \textit{Resample} from $\x$ be sampling values in $\x$ uniformly, with repetition. Since we use repetition the \textit{Resample}'s size is independent of the size of $\x$ (Although it makes little sense for it to be greater than $|\x|$).\\

\definition{Bootstrapping}
\textit{Bootstrapping} is the process of generating multiple \textit{Resamples} of a data set \& then estimating a parameter value for each of these \textit{resamples}. These estimated values can then be assessed.\\

\example{Bootstrapping}
The algorithm below describes how to perofrm a \textit{Bootstrapping} operation for the mean of a given data set $\x$. It produces $m$ \textit{resamples} of size $n$ from $\x$ and returns a $95\%$ \textit{Confidence Interval} for the estimated means of these samples.\\
\begin{algorithm}[H]
\SetKwInOut{Require}{require}
\caption{Estimating Mean using Bootstrapping}
\Require{$\x\ \{\text{data set}\}$}
$\mu s=\{\}\ \{$resample means$\}$\\
$\mu s$ append $mean(\x)$\\
\For{$i=0\dots m$} {
$x_i\leftarrow sample(\x,n,$replace=$TRUE)$\\
$\mu s$ append $mean(x_i)$
}
\Return{$quantile(\mu s,(0.025,0.0975)$}
\end{algorithm}

\section{Inference for Linear Models}

\definition{Linear Model}
A \textit{Linear Model} is a mathematical model where the \textit{response vector}, $\textbf{y}$, is linear wrt some parameters $\pmb\beta$ and zero-mean \textit{random error} $\pmb\varepsilon$.
$$\textbf{y}=X\pmb\beta+\pmb\varepsilon$$
where $X$ is the \textit{Model Matrix} (\ie observed data).\\
Usually we assume $\pmb\varepsilon\sim\text{Normal}(0,\sigma^2 I)$ although the normality assumption is less important as the \textit{Central Limit Theorem} typically takes care of any issues.\\

\definition{Model Matrix}
A \textit{Model Matrix}, $X$, is the set of values observed in a system. Rows are read as a single observation \& columns as a single \textit{Predictor Varaible}.\\
The \textit{Predictor Variables} fulfil one of the following roles
\begin{itemize}
	\item[-] \textit{Metric} - Quantifable measurement from the system.
	\item[-] \textit{Factor} - A categorisation. Typically take the a binary value ($0,1$) to represent whether an observation fits a given category or not.
\end{itemize}

\remark{Only the parameters of a Linear model need to be linear. The predictor variables can be composed in any way deemed fit.}
$y=\alpha x^2+\varepsilon$ is valid but $y=\alpha^2x+\varepsilon$ is not.\\

\example{Formulating Linear Model}
The following is a linear model for a system with \textit{Metrics} $x_i\ \&\ z_i$ and \textit{Factor} $g_i$.
$$y_i=\gamma_{g_i}+\alpha_1x_i+\alpha_2z_i+\alpha_4z_i^2+\alpha_4z_ix_i+\varepsilon_i$$
where $\gamma_{g_i}$ is the parameter for category represented by $g_i$.\\
We can describe the system about in terms of matrices
$$\begin{pmatrix}y_1\\y_2\\\vdots\\y_n\end{pmatrix}=\begin{pmatrix}1&0&0&x_1&z_1&z_1^2&z_1x_1\\0&0&1&x_2&z_2&z_2^2&z_2x_2\\\vdots&\vdots&\vdots&\vdots&\vdots&\vdots&\vdots\\0&1&0&x_n&z_n&z_n^2&z_nx_n\end{pmatrix}\begin{pmatrix}
\gamma_1\\\gamma_2\\\gamma_3\\\alpha_1\\\alpha_2\\\alpha_3\\\alpha_4\end{pmatrix}+\begin{pmatrix}\varepsilon_1+\varepsilon_2\\\vdots\\\varepsilon_n\end{pmatrix}$$
In the above formulation $y_1$ fulfils category $1$, $y_2$ fulfils $3$ and $y_n$ fulfils $2$.\\

\example{Linear Model}
Consider a data set for the stopping $distance$ of a car with \textit{Predictor Variable} $speed$ at the point at which the signal to stop is given.\\
By considering basic physics we can theorise the following model
\[\begin{array}{rcl}
distance_i&=&\beta_1 speed_i+\beta_2 speed_i^2+\varepsilon_i\\
&=&\text{Thinking}+\text{Loss Kinetic Energy}+\text{Error}
\end{array}\]
where $\varepsilon_i\iid\text{Normal}(0,\sigma^2)$.\\
Suppose we want to test whether to make the model more flexible. We can theorise the following model \& test whether $\beta_0=0=\beta_3$ (as expected).
$$distance_i=\beta_0+\beta_1 speed_i+\beta_2 speed_i^2+\beta_3speed_i^3+\varepsilon_i\\$$

\subsection{Linear Model Estimation \& Checking}

\proposition{Frequentist Approach}
In the \textit{Frequentist Approach} to \textit{Linear Models} we assume that $\pmb\beta$ and $\sigma^2$ are fixed states of nature, although they are unknown to us, and all randomness is inherited from the random variability in the data. We want to find a point estimate for $\pmb\beta$ which minimises the \textit{Residual Sum of Squares}.\\

\definition{Residual Sum of Squares}
Let $(X,\textbf{y})$  be a set of training data \& $\pmb\beta$ a \textit{Parameter Vector}.\\
The \textit{Residual Sum of Squares} is the square difference between our estimate for the \textit{Response Variable} and its true value.
$$S:=\sum_{i=1}^n(y_i-\mu_i)^2=\|\textbf{y}-\pmb\mu\|^2\text{ where }\pmb\mu=X\pmb\beta$$

\proposition{Least Squares for Linear Model}
From the definition of \textit{Residual Sum of Squares} as the \textit{Euclidian Distance} between the response \& estimated vectors we note that its value is unchanged if we reflect or rotate $(\textbf{y}-\pmb\mu)$.\\
Next we note that any real matrix, $X\in\reals(n\times p)$, can be decomposed into
$$X=\mathcal{Q}\begin{pmatrix}R\\0\end{pmatrix}=QR\text{ note that }\mathcal{Q}\neq Q$$
where $R\in\reals(p\times p)$ is an \textit{Upper Triangular Matrix} and $\mathcal{Q}\in\reals(n\times n)$ is an \textit{Orthogonal Matrix}, the first $p$ columns of which form $Q$.\\
Since $\mathcal{Q}$ is \textit{Orthogonal} we have that $\mathcal{Q}^T\mathcal{Q}=I$.\\
We can now derive the result that
\[\begin{array}{rcl}
\|\textbf{y}-X\pmb\beta\|^2&=&\|\mathcal{Q}^T\textbf{y}-\mathcal{Q}^TX\pmb\beta\|^2\\
&=&\left\|\mathcal{Q}^T\textbf{y}-\begin{pmatrix}R\\0\end{pmatrix}\pmb\beta\right\|^2\\
&=&\left\|\begin{pmatrix}\textbf{f}\\\textbf{r}\end{pmatrix}-\begin{pmatrix}R\\0\end{pmatrix}\right\|^2\text{ where }\begin{pmatrix}\textbf{f}\\\textbf{r}\end{pmatrix}\equiv\mathcal{Q}^T\textbf{y}\\
&=&\|\textbf{f}-R\pmb\beta\|^2+\|\textbf{r}\|^2
\end{array}\]
Thus minimising the \textit{Residual Sum of Squares} is reduced to choosing $\pmb\beta$ st $R\pmb\beta=\textbf{f}$.\\
Hence, provided that $X$ and $R$ have full rank
$$\hat{\pmb\beta}_\text{LS}=R^{-1}\textbf{f}$$
\nb After choosing $\pmb\beta$ we have that the \textit{Residual Sum of Squares} is just $\|\textbf{r}\|^2$.

\proposition{$\hat{\pmb\beta}_\text{LS}$ is Unbiased}
We have that
\[\begin{array}{rcl}
\expect(\hat{\pmb\beta})&=&\expect(\textbf{R}^{-1}\textbf{Q}^T\textbf{y})\\
&=&\textbf{R}^{-1}\textbf{Q}^T\expect(\textbf{y})\\
&=&\textbf{R}^{-1}\textbf{Q}^T\textbf{X}\pmb\beta\\
&=&\textbf{R}^{-1}\textbf{Q}^T\textbf{Q}\textbf{R}\pmb\beta\\
&=&\pmb\beta
\end{array}\]
Thus $\hat{\pmb\beta}_\text{LS}$ is unbiased.\\

\proposition{Variance of $\hat{\pmb\beta}_\text{LS}$}
We have $\Sigma_\textbf{y}=I\sigma^2$.\\
Thus $\Sigma_\textbf{f}=\textbf{Q}^T\textbf{Q}\Sigma_\textbf{y}=\textbf{Q}^T\textbf{Q}I\sigma^2=I\sigma^2$.\\
Hence
$$\Sigma_{\hat\beta}=\textbf{R}^{-1}\textbf{R}^{-T}\sigma^2$$

\remark{Checking}
In order to make inferences beyond estimating $\beta$ we need to check that our assumptions about $\varepsilon_i$ still hold.\\
We can estimate these values as $\hat\varepsilon_i=y_i-\hat\mu_i$ where $\hat{\pmb\mu}=\textbf{X}\hat{\pmb\beta}$.\\
Plotting these estimates, $\hat\varepsilon_i$, against fitted values, $\hat\mu_i$, allows us to look for systematic patterns in the mean of residuals, which would indicate a violation of the independence assumption

\subsection{Gauss-Markov Theorem}

\remarkk{Alternatives to Least-Squares Estimates}
\begin{itemize}
	\item[-] We may wish to find an estimate of $\beta$ which is as close to the real value as possible, so minimising $\|\hat\beta-\beta\|^2$. However it is possible the data gives a lot of information about $\beta_i$ but little about $\beta_j$, does it make sense to weight these equally.
	\item[-] We could only allow \textit{unbiased estimators}, ie $\expect(\hat\beta)=\beta$. And then among those choose the one with least variance.
\end{itemize}

\theorem{Gauss-Markov Theorem}
Define $\pmb\mu:=\expect(\textbf{Y})=\textbf{X}\pmb\beta$ and $\Sigma_y=\sigma^2I$.\\
Let $\tilde\phi=\textbf{c}^T\textbf{Y}$ be any unbiased linear estimator of $\phi=\textbf{t}^T\pmb\beta$ where $\textbf{t}$ is an arbitrary vector. Then
$$\var(\tilde\phi)\geq\var(\hat\phi)\text{ where }\hat\phi=\textbf{t}^T\hat{\pmb\beta_\text{LS}}\ \&\ \hat{\pmb\beta}_\text{LS}=\textbf{R}^{-1}\textbf{Q}^T\textbf{Y}$$
Since $\textbf{t}$ is arbitraru, this implies that each element of $\hat{\pmb\beta}$ is a minimum variance unbiased estimator.\\

\proof{Gauss-Markov Theorem}
Since $\tilde\phi$ is a linear transformation of $\textbf{Y}$, $var(\tilde\phi)=\textbf{c}^T\textbf{c}\sigma^2$.\\
To compare the variances of $\hat\phi$ and $\tilde\phi$ it is useful to express $\var(\hat\phi)$ in terms of $\textbf{c}$.\\
Because $\tilde\phi$ is unbiased we have
\[\begin{array}{rrcl}
&\expect(\textbf{c}^T\textbf{Y})&=&\textbf{t}^T\pmb\beta\\
\implies&\textbf{c}^T\expect(\textbf{Y})&=&\textbf{t}^T\pmb\beta\\
\implies&\textbf{c}^T\textbf{X}\pmb\beta&=&\textbf{t}^T\pmb\beta\\
\implies&\textbf{c}^T\textbf{X}&=&\textbf{t}^T
\end{array}\]
So the variance of $\hat\phi$ can be written as
$$\var(\hat\phi)=\var(\textbf{t}^T\hat{\pmb\beta})=\var(\textbf{c}^T\textbf{X}\hat{\pmb\beta})=\var(\textbf{c}^T\textbf{QR}\hat{\pmb\beta})$$
This is the variance of a linear transformation of $\hat{\pmb\beta}$ and the covariance matrix of $\hat{\pmb\beta}$ is $\textbf{R}^{-1}\textbf{R}^{-T}\sigma^2$.\\
Thus
$$\var(\hat\phi)=\var(\textbf{c}^T\textbf{QR}\hat{\pmb\beta})=\textbf{c}^T\textbf{QRR}^{-1}\textbf{R}^{-T}\textbf{R}^T\textbf{Q}^T\textbf{c}^T\sigma^2=\textbf{c}^T\textbf{QQ}^T\textbf{c}\sigma^2$$
Hence
$$\var(\tilde\phi)-\var(\hat\phi)=\textbf{c}^T(I-\textbf{QQ}^T)\textbf{c}\sigma^2$$
Because the columns of $\textbf{Q}$ are orthogonal, $\textbf{QQ}^T=\textbf{QQ}^T\textbf{QQ}^T$ it follows that
$$\textbf{c}^T(I-\textbf{QQ}^T)\textbf{c}=[(I-\textbf{QQ}^T)\textbf{c}]^T(I-\textbf{Q}\textbf{Q}^T)\textbf{c}\geq0$$
since this is just the sum of squares of the elements of teh vector $(I-\textbf{QQ}^T)\textbf{c}$.\proved

\remark{Least Squares Variance}
Amongst unbiased and linear estimators in $\textbf{Y}$, least squares estimators have minimum variance.\\
It is still possible that some non-linear estimator might be even better.

\subsection{Further Inference on Linear Models}

\remark{Requirements}
In order to make further inferences about linear models (\eg confidence intervals \& hypothesis testing) we need to make our model completely probabilistic, since these inferences are probabilistic concepts.\\
This requires us to specify a full distribution for the error $\pmb\varepsilon$.\\
We assume
\[\begin{array}{rcl}
\pmb\varepsilon&\iid&\text{Normal}(0,I\sigma^2)\\
\implies\textbf{y}&\sim&\text{Normal}(\textbf{X}\beta,I\sigma^2)\\
\implies\hat{\pmb\beta}&\sim&\text{Normal}(\pmb\beta,\Sigma_{\hat\beta})\\
\text{where }\Sigma_{\hat\beta}&=&R^{-1}R^{-T}\sigma^2
\end{array}\]

\theorem{$\dfrac{\hat\beta_i-\beta_i}{\hat\sigma_{\hat\beta_i}}\sim t_{n-p}$}

\proof{Theorem 3.2}
$\pmb{\mathcal{Q}}^T\textbf{y}$ is a linear transformation of a normal random vector, so is a normal random vector with covariance matrix
$$\Sigma_{\pmb{\mathcal{Q}}^T\textbf{y}}=\pmb{\mathcal{Q}}^TI\pmb{\mathcal{Q}}\sigma^2=I\sigma^2$$
The elements of $\pmb{\mathcal{Q}}^T\textbf{y}$ are mtually independent. Further
\[\begin{array}{rcl}
\expect\left[\begin{pmatrix}\textbf{f}\\\textbf{r}\end{pmatrix}\right]&=&\expect[\pmb{\mathcal{Q}}^T\textbf{y})\\
&=&\pmb{\mathcal{Q}}^T\textbf{X}\pmb\beta\\
&=&\begin{pmatrix}\textbf{R}\\\pmb0\end{pmatrix}\pmb\beta\\
\implies\expect(\textbf{f})&=&\textbf{R}\pmb\beta\\
\text{and }\expect(\textbf{r})&=&\pmb0
\end{array}\]
Thus
$$\textbf{f}\sim\text{Normal}(\textbf{R}\pmb\beta,I_p\sigma^2)\text{ and }\textbf{r}\sim\text{Normal}(0,I_{n-p}\sigma^2)$$
Now we can deduce
\[\begin{array}{rrcl}
&r_i&\overset{\text{ind}}{\sim}&\text{Normal}(0,\sigma^2)\\
\implies&\frac{r_i}{\sigma}&\sim&\text{Normal}(0,1)\\
\implies&\displaystyle{\sum_{i=1}^{n-p}\left(\frac{r_i}{\sigma}\right)^2}&\sim&\chi^2_{n-p}
\end{array}\]
Since $\expect(\chi^2_{n-p})=n-p$ we have that $\hat\sigma^2=\frac1{n-P}\|\textbf{r}\|^2$ is an unbiased estimator.\\
Let $\sigma_{\hat\beta_i}=\sqrt{\Sigma_{\hat\beta_i}(i,i)}$ then $\hat\sigma_{\hat\beta_i}=\sqrt{\hat\Sigma_{\hat\beta_i}(i,i)}$ but $\hat\Sigma_{\hat\beta_i}=\Sigma_{\hat\beta_i}\frac{\hat\sigma^2}{\sigma^2}\implies\hat\sigma_{\hat\beta_i}\frac{\hat\sigma}{\sigma}$.\\
Consider
\[\begin{array}{rcl}
\dfrac{\hat\beta_i-\beta_i}{\hat\sigma_{\beta_i}}&=&\dfrac{\hat\beta_i-\beta_i}{\sigma_{\hat\beta_i}\hat\sigma/\sigma}\\
&=&\dfrac{(\hat\beta_i-\beta_i)/\sigma_{\hat\beta_i}}{\sqrt{\frac1{\sigma^2}\|\textbf{r}\|^2/(n-p)}}\\
&\sim&\dfrac{\text{Normal}(0,1)}{\sqrt{\chi^2_{n-p}/(n-p)}}\\
&\sim&t_{n-p}
\end{array}\]

\proposition{Confidence Intervals for $\beta_i$}
Supose we want a $(1-2\alpha)100\%$ confidence interval for $\beta_i$.\\
Then
\[\begin{array}{rcl}
\prob\left(-t_{n-p}(\alpha)<\dfrac{\hat\beta_i-\beta_i}{\hat\sigma_{\hat\beta_i}}<t_{n-p}(\alpha)\right)&=&\prob\left(\hat\beta_i-t_{n-p}(\alpha)\sigma_{\hat\beta_i}<\beta_i<\hat\beta_i+t_{n-p}(\alpha)\sigma_{\hat\beta_i}\right)\\
&=&1-2\alpha
\end{array}\]
where $\prob(t_{n-p}(\alpha)\geq t_{n-p})=1-\alpha$.\\

\subsection{Geometry of Linear Models}

\remark{Least Squares Estimation as Geometry}
\textit{Least Squares Estimation} of linear models is the same as finding the orthogonal projection of the response vector $\textbf{y}\in\reals^n$ onto the $p$-dimensional linear subspace spanned by the columns of $\X\in\reals^{n\times p}$.\\
By the linear model $\expect(\textbf{y})$ lies in the space spanned by all possible linear combinations of the columns of $\X$ \& least squares find the point in that space that is cloests to $\textbf{y}$ in \textit{Euclidean Distance}.\\

\remark{Projection Matrix}
Consider the \textit{Projection Matrix} that maps the response data $\textbf{y}$ to the fitted values $\hat{\pmb\mu}$.\\
We have that
$$\hat{\pmb\mu}=\X\hat{\pmb\beta}=\textbf{QRR}^{-1}\textbf{Q}^T\textbf{y}=\textbf{QQ}^T\textbf{y}$$
Thus the projection matrix is $\textbf{A}=\textbf{QQ}^T$.\\
\nb Often $\textbf{A}$ is referred to as the \textit{Influence Matrix} or \textit{Hat Matrix}.\\

\proposition{Projection Matrix Idempotent}
Let \textbf{A} be the \textit{Projection Matrix} of a \textit{Linear Model}.\\
$\textbf{A}$ is said to be \textit{Idempotent} since $\textbf{A}=\textbf{AA}$.\\
This is since the orthogonal projection of $\hat{\pmb\mu}$ onto the column space of $\X$ must be $\hat{\pmb\mu}$.\\

\subsection{Results in terms of Model Matrix, $\X$}

\propositionn{Results in terms of Model Matrix, $\X$}
\[\begin{array}{rclcrclcrcl}
\Sigma_{\hat\beta}&=&(\X^T\X)^{-1}\sigma^2&\quad&\hat{\pmb\beta}&=&(\X^T\X)^{-1}\X^T\textbf{y}&\quad&\textbf{A}&=&\X(\X^T\X)^{-1}\X^T\\
&=&(\textbf{R}^T\textbf{Q}^T\textbf{QR})^{-1}\sigma^2&&&=&\textbf{R}^{-1}\textbf{R}^{-T}\textbf{R}^T\textbf{Q}^T\textbf{y}\\
&=&(\textbf{R}^T\textbf{R}^{-1}\sigma^2&&&=&\textbf{R}^{-1}\textbf{Q}^T\textbf{y}\\
&=&\textbf{R}^{-1}\textbf{R}^{-T}\sigma^2&&&=&\textbf{R}^{-1}\textbf{f}\\
\end{array}\]

\subsection{Bayesian Analysis}

\remark{Bayesian Analysis of Linear Models}
To perfor a full \textit{Bayesian Analysis} of a \textit{Linear Model} we need to define prior distributions for $\pmb\beta$ and $\sigma^2$. Typically
In order to make this problem analytically tractable we use conjugate priors. Conjugacy can be used for defining
$$\pmb\beta\sim\text{Normal}(\pmb\beta_0,\pmb\psi^{-1})\quad\text{and}\quad\tau\sim\Gamma(a,b)$$
where $tau:=\frac1{\sigma^2}$ is precision measure.\\
Here $a,b,\pmb\beta_0\text{ and }\pmb\psi$ are quantities which we need to define values for, for practical analysis.\\
This gives us the following distributions
\[\begin{array}{rcl}
f(\textbf{y},\pmb\beta,\tau)&\propto&{\displaystyle\tau^{n/2}e^{-\frac{\tau}{2}\|\textbf{y}-\X\pmb\beta\|^2}e^{-\frac12(\pmb\beta-\pmb\beta_0)^T\pmb\psi(\pmb\beta-\pmb\beta_0)}e^{-b\tau}\tau^{a-1}}\\
f(\tau|\pmb\beta,\textbf{y})&\propto&{\displaystyle\tau^{\frac{n}2+a-1}e^{-\tau(b+\frac12\|\textbf{y}-\X\pmb\beta\|^2)}}\\
&\sim&{\displaystyle\Gamma(\frac{n}2+a,b+\frac12\|\textbf{y}-\X\pmb\beta\|^2)}\\
f(\pmb\beta|\tau,\textbf{y})&\propto&e^{-\frac12(\pmb\beta^T\X^T\X\pmb\beta\tau-2\beta\X^T\textbf{y}\tau+\pmb\beta^T\pmb\psi\pmb\beta-2\pmb\beta^T\pmb\psi\pmb\beta_0)}\\
&\propto&e^{-\frac12[\pmb\beta-(\X^T\X\tau+\pmb\psi)^{-1}(\tau\X^T\textbf{y}+\pmb\psi\pmb\beta_0)]^T(\X^T\X\tau+\pmb\psi)\pmb\beta-(\X^T\X\tau+\pmb\psi)^{-1}(\tau\X^T\textbf{y}+\pmb\psi\pmb\beta_0)]}\\
&\sim&\text{Normal}[(\X^T\X\tau+\pmb\psi)^{-1}(\tau\X^T\textbf{y}+\pmb\psi\pmb\beta+0),(\X^T\X\tau+\pmb\psi)^{-1}]
\end{array}\]
If either the sample size tends to infinity (\ie $n\to\infty$) or the prior precision matrix tends to the zero matrix then
$$f(\pmb\beta|\tau,\textbf{y})\overset\to\sim\text{Normal}(\hat{\pmb\beta},(\X^T\X)^{-1}\sigma^2)$$
\nb We have not produced the joint distribution $\pmb\beta,\tau|\textbf{y}$ but just two conditionals.\\

\remark{Proceeding from Conditionals}
There are a few options to proceed from the results in \textbf{Remark 3.8}
\begin{enumerate}
	\item Iteratively find the posteior modes of $\pmb\beta$ given teh estiamte mode of $\tau$ and the posterior mode of $\tau$ given the estimated modes of $\pmb\beta$ until the mode of $\tau$ connverges.\\
	Then plug this into the conditional density of $\pmb\beta$.
	\item Integrate $\pmb\beta$ out of $f(\tau|\pmb\beta,\textbf{y})$ to obtain the marginal likelihood $f(\tau|\textbf{y})$ which can be maximised to find $\hat\tau$.\\
	$\hat\tau$ can be plugged into $f(\pmb\beta|\tau,\textbf{y})$.
	\nb Also known as \textit{Empirical Bayes}.
	\item Alternate simulate of $\pmb\beta$ from $f(\pmb\beta|\tau,\textbf{y})$ given $tau$ with simulation from $f(\tau|\pmb\beta,\textbf{y})$, given the last simulated $\pmb\beta$, to generate joint draws of $\tau\ \&\ \pmb\beta$ from $f(\pmb\beta,\tau|\textbf{y})$.\\
	\nb Also known as \textit{Gibbs Sampling}.
\end{enumerate}

\section{Causality, Confounding \& Randomisation}

\definition{Causality}
\textit{Causality} is a problem in statistical inference where we wish to find out which variables affect a particular variable, and are mearly correlated. This is more difficult that other forms of inference, but is useful in many real world scenarios especially in science \& economics.\\

\example{Causation \& Correlation}
There is an observed correlation between birth rates in Europe \& stork populations. There is no causation between the two, however it is likely that increased industrialisation led to the decrease in both since it lead to more healthcare for humans \& less habitats for storks.\\

\subsection{Controlled Experiments and Randomisation}

\definition{Hidden Variables}
\textit{Hidden Variables} are variables which likely effect a system but which we can/do not observed.\\

\definition{Randomisation}
\textit{Randomisation} is the process of splitting subjects into different groups. Typically a control \& an active group. This is meant to break correlation between observed \& hidden variables.\\

\remark{Hidden Variables}
Consider the scenario where we wish to test whether exercise influences fat mass. It is likely that ther are lots of other factors. These factors will correlate with both exercise \& fat mass. By splitting subjects into a control \& exercise groups at random we break the correlation of these other features but not that between fat mass \& exercise. The other factors are now random error.\\

\proposition{Formalisation of Hidden Variables}
Consider the true model matrix $(X,H)$ where $X$ is the observed variables \& $H$ is the hidden variables. We assume that the columns of $H$ have mean 0.\\
We have
$$\tilde\beta_X=(X^TX)^{-1}X^T\textbf{y}\text{ for assumed model }y=X\beta_X+\varepsilon$$
If we knew $H$ then we would have
$$\begin{pmatrix}\hat\beta_X\\\hat\beta_H\end{pmatrix}=\begin{pmatrix}X^X&X^TH\\H^TX&H^TH\end{pmatrix}^{-1}\begin{pmatrix}X^T\\H^T\end{pmatrix}\textbf{y}\text{ for true model }\textbf{y}=X\beta_X+H\beta_H+\varepsilon$$
Since $X^TH\neq0\implies\tilde\beta_X\neq\hat\beta_X$.\\
The randomised allocation to groups is used to try and make $X^TH=0$.\\

\remark{Randomised Tests}
Sometimes it is frowned upon (ethically) to perform random tests. Such as testing if high levels of alcohol consumtion is correlated with heart disease.\\
\nb There is a reason China is becoming such an advanced country.

\subsection{Instrumental Variables}

\definition{Instrumental Variable}
An \textit{Instrumental Variable}, $Z$, is used in regression analysis when there are \textit{hidden variables} in the model. \textit{Instrumental Variables} are correlated with the \textit{Explanatory Variables}, $X$ but uncorrelated with the error term $\textbf{e}$ in the model $\textbf{y}=X\beta+\textbf{e}$.\\

\proposition{Without Instrumental Variables}
Consider the true model $\textbf{y}=X\beta_X+H\beta_H+\pmb\varepsilon$ with $H$ being the hidden variables with columns centred at 0.\\
Suppose we wish to fit the model $\textbf{y}=X\beta_X+\textbf{e}$.\\
In this case $\textbf{e}=H\beta_H+\varepsilon$ and likely does not fulfil the criteria of linear model random error.\\
We have
\[\begin{array}{rcl}
\expect(\hat\beta_X)&=&(X^TX)^{-1}X^T\begin{pmatrix}X&H\end{pmatrix}\begin{pmatrix}\beta_X\\\beta_H\end{pmatrix}\\
&=&\beta_X+(X^TX)^{-1}X^TH\beta_H\\
&\neq&\beta_x\text{ since }X\perp\textbf{e}\text{ and thus }X^TH\neq0
\end{array}\].\\

\proposition{With Instrumental Variables}
Let $Z$ be an instrumental variable (\ie it is correlated with $X$ but not with $H$).\\
Assume that $\text{rank}(Z)\geq\text{rank}(X)$ and $Z$'s columns are centred around 0.\\
Project $X$ onto column space of $Z$
$$X\mapsto A_ZY\text{ where }\underbrace{A_Z=Z(Z^TZ)^{-1}Z^T}_\text{Projection Matrix}$$
Now use $A_ZX$ as the model matrix
\[\begin{array}{rcl}
\hat\beta_X&=&(X^TA_ZX)^{-1}X^TA_z\textbf{y}\\
\expect(\textbf{y})&=&\begin{pmatrix}X&H\end{pmatrix}\begin{pmatrix}\beta_X\\\beta_H\end{pmatrix}\\
\expect(\hat\beta_X)&=&(X^TA_ZX)^{-1}X^TA_ZX\beta_X+(X^TA_ZX)^{-1}X^T\underbrace{A_ZH}_{\approx0}\beta_H\\
&=&\beta_X+0\\
&=&\beta
\end{array}\]
Thus this $\hat\beta_X$ is unbiased.\\
\nb $A_ZH\approx0$ since $Z$ and $H$ are uncorrelated.

\newpage
\setcounter{section}{-1}
\section{Reference}

\subsection{Definitions}

\definition{Heavy Tailed}

\definition{Censored Data}

\definition{Upper Triangular Matrix}

\definition{Orthogonal Matrix}

\definition{$p$-Value}

\definition{Euclidean Distance}

\subsection{Probability}

\definition{Random Variable}
A \textit{Random Variable} is a function from the sample space to the reals.
$$X:\Omega\to\reals$$
\textit{Random Variables} take a different value each time they are observed and thus we define distributions for the probability of them taking particular values.\\
\textit{Random Variables} form the basis of models.\\

\definition{Cummulative Distribution}
The \textit{Cummulative Distribution} function of a \textit{Random Variable}, $X$, is the function $F_X(\cdot)$ st
\[\begin{array}{rllrl}
F_X(\cdot)&:&\reals\to[0,1]\\
F_X(x)&:=&\prob(X\leq x)&=&\displaystyle\sum_{i=-\infty}^x\prob(X=i)\\
&&&=&\displaystyle\int_{-\infty}^xf_X(x)dx
\end{array}\]
The \textit{Cummulative Distribution} is a monotonic function.\\

\remark{Continuous Cummulative Distribution}
If a \textit{Cummulative Distribution} is \textit{continuous} then $F_X(X)\sim\text{Uniform}[0,1]$.\\

\prooff{Remark 2.1}
\[\begin{array}{rcl}
F(X)&=&\prob(X\leq x)\\
&=&\prob(F(X)\leq F(x))\\
\implies\prob(F(X)\leq u)&=&u\text{ if $F$ is continuous}
\end{array}\]

\definition{Quantile Function}
The \textit{Quantile Function} of a \textit{Random Variable} is the inverse function of the \textit{Cumulative Distribution}.\\
\[\begin{array}{rll}
F^-_X(\cdot)&:&[0,1]\to\reals\\
F^-_X(u)&:=&\min\{x:F(x)\geq u\}
\end{array}\]
If a distribution has a computable \textit{Quantile Function} then we are able to generate random variable values by sampling from a uniform distribution \& then passing that value into the \textit{Quantile Function}.\\

\definition{(Q-Q) Plot}
Consider a data set $\{x_1,\dots,x_n\}$.\\
A \textit{(Q-Q) Plot} of this data set plots the ordered data set, $\{x_{(1)},\dots,x_{(n)}\}$, against the theoretical quantiles $F^-\left(\frac{i-.5}n\right)$.\\
The close this line is to $y=x$ the more likely it is the data was generated by this \textit{Cummulative Distribtion}.\\
\nb AKA \textit{Quantile-Quantile Plot}

\definition{Probabiltiy Mass Function}
A \textit{Probability Mass Function} returns the probabiltiy of a \underline{discrete} random variable taking a particular value.
\[\begin{array}{rll}
f_X(\cdot)&:&\reals\to[0,1]\\
f_X(x)&:=&\prob(X=x)\\
\end{array}\]

\definition{Probability Density Function}
Since the probabiltiy of a \textit{Continuous Random Variable} taking a specific value is zero we cannot use the \textit{Probability Mass Function}.
\[\begin{array}{rll}
f_X(\cdot)&:&\reals\to[0,1]\\
\prob(a\leq X\leq b)&=&\displaystyle\int_a^bf(x)dx
\end{array}\]
\nb $F_X'(x)=f(x)$ when $F_X'(\cdot)$ exists.\\

\definition{Joint Probabiltiy Density Function}
Let $X\ \& Y$ be \textit{Random Variables}.\\
The \textit{Joint Probabiltiy Density Function} of $X$ and $Y$ is the function $f_{X,Y}(x,y)$ st
$$\prob((X,Y)\in\Omega)=\iint_\Omega f_{X,Y}(x,y)dxdy$$
\nb This can be seen as evaluation $\Omega$ in the $X-Y$ plane.\\

\definition{Marginal Distribution}
Let $X\ \&\ Y$ be \textit{Random Variables} with \textit{Joint Probability Density} $f_{X,Y}(\cdot,\cdot)$.\\
We can find the \textit{Marginal Distribution} of $X$ by evaluating the $f_{X,Y}$ at each value wrt $Y$.
$$f_X(x)=\int_{-\infty}^\infty f_{X,Y}(x,y)dy$$

\definition{Expected Value, $\expect$}
The \textit{Expected Value} of a \textit{Random Variable}, $X$, is its mean value.
\[\begin{array}{rcll}
\expect(X)&:=&\displaystyle\int_{-\infty}^\infty xf(x)dx&\text{ [Continuous]}\\
\expect(g(X))&:=&\displaystyle\int_{-\infty}^\infty g(x)f(x)dx&\\\\
\expect(X)&:=&\displaystyle\sum_{-\infty}^\infty xf(x)&\text{ [Discrete]}\\
\expect(g(X))&:=&\displaystyle\sum_{-\infty}^\infty g(x)f(x)&
\end{array}\]

\remarkk{Linear Transformations of Expected Value}
$$\expect(a+bX)=a+b\expect(X)\text{ where }a,b\in\reals$$

\remark{Expected Value of Composed Random Variables}
Let $X$ \& $Y$ be \textit{Random Variables}. Then
$$\expect(X+Y)=\expect(X)+\expect(Y)$$
If $X$ \& $Y$ are \textit{independent}. Then
$$\expect(XY)=\expect(X)\expect(Y)$$

\proof{Remark 2.3}
\[\begin{array}{rcl}
\expect(X+Y)&=&\int(x+y)f_{X,Y}(x,y)dxdy\\
&=&\int xf_{X,Y}(x,y)dxdy+\int yf_{X,Y}(x,y)dxdy\\
&=&\expect(X)+\expect(Y)\\
\expect(XY)&=&\int xyf_{X,Y}(x,y)dxdy\\
&=&\int xf_X(x)yf_Y(y)dxdy\text{ by independence}\\
&=&\int xf_X(x)dx\int yf_Y(y)dy\\
&=&\expect(X)\expect(Y)
\end{array}\]

\definition{Variance, $\sigma^2$}
The \textit{Variance} of a \textit{Random Variable}, $X$, is a measure of its spread around its expected value.
$$\var(X)=\expect[(X-\expect(X))^2]=\expect(X^2)-\expect(X)^2$$

\remarkk{Linear Transformations of Variance}
$$\var(a+bX)=b^2\var(X)\text{ where }a,b\in\reals$$

\prooff{Remark 2.4}
\[\begin{array}{rcl}
\var(a+bX)&=&\expect[((a+bX)-(a-b\mu))^2]\\
&=&\expect[b^2(X-\mu)^2]\\
&=&b^2\expect[(X-\mu)^2]\\
&=&b^2\var(X)
\end{array}\]

\definition{Co-Variance}
\textit{Co-Variance} is a measure of the joint variability of two \textit{Random Variables}.
$$\cov(X,Y):=\expect[(X-\expect(X))(Y-\expect(Y))]=\expect(XY)-\expect(X)\expect(Y)$$
\nb If $X\ \&\ Y$ are independent then $\cov(X,Y)=0$ since $\expect(XY)=\expect(X)\expect(Y)$.\\
\nb $\cov(X,Y)=\cov(Y,X)$.\\

\definition{Co-Variance Matrix, $\Sigma$}
Let $\textbf{X}:=\{X_1,\dots,X_n\}$ be a set of random variables.\\
A \textit{Co-Variance Matrix} describes the \textit{Variance} \& \textit{Co-Variance} of each combination of \textit{Random Variables} in $\textbf{X}$.\\
$$\Sigma:=\expect[(\textbf{X}-\pmb\mu)(\textbf{X}-\pmb\mu)^T]$$
\nb $\Sigma_{ii}=\var(X_i)\ \&\ \Sigma_{ij}=\cov(X_i,X_j)$ for $i\neq j$. $\Sigma$ is symmetric.\\

\remarkk{Linear Transformation of Covariance}
$$\Sigma_{AX+b}=A\Sigma A^T$$

\prooff{Remark 2.5}
\[\begin{array}{rcl}
\Sigma_{AX+b}&=&\expect[(AX+\textbf{b}-A\pmb\mu-\textbf{b})(AX+\textbf{b}-A\pmb\mu-\textbf{b})^T]\\
&=&\expect[(AX-A\pmb\mu)(AX-A\pmb\mu)^T]\\
&=&A\expect[(X-\pmb\mu)(X-\pmb\mu)^T]A^T\\
&=&A\Sigma A^T
\end{array}\]

\definition{Conditional Distribution}
Let $X\ \&\ Y$ be \textit{Random Variables} with \textit{Joint Probability Density} $f_{X,Y}(\cdot,\cdot)$.\\
Suppose we know that $Y$ takes the value $y_0$ \& we wish to establish the probability of $X$ taking the value $x$.
$$f(X=x|Y=y_0)=\dfrac{f_{X,Y}(x,y_0)}{f_Y(y_0)}$$
assuming $f(y_0)>0$.\\

\proof{Conditional Distribution}
We expect $f(X=x|Y=y_0)=kf_{X,Y}(x,y_0)$ for some constant $k$.\\
We know that for $kf_{X,Y}(x,y_0)$ to be a valid distribution it must integrate to one.
\[\begin{array}{rrcl}
&k\displaystyle\int_{-\infty}^\infty f_{X,Y}(x,y_0)dx&=&1\\
\implies&kf_Y(y_0)&=&1\\
\implies&k&=&\dfrac{1}{f_Y(y_0)}\\
\implies&f(X=x|Y=y_0)&=&\dfrac{f_{X,Y}(x,y_0)}{f_Y(y_0)}
\end{array}\]

\propositionn{Conditional Distributions with Three Random Variables}
\[\begin{array}{rcl}
f(x,z|y)&=&f(x|z,y)f(z|y)\\
f(x,y,z)&=&f(x|y,z)f(z|y)f(y)\\
&=&f(x|y,z)f(y,z)
\end{array}\]

\definition{Independent Random Variables}
Let $X$ \& $Y$ be random variables.\\
$X$ \& $Y$ are said to be \textit{Statistically Independent} if the \textit{Conditional Distribution} $f(x|y)$ is independent of $y$.\\
Thus
\[\begin{array}{rcl}
f(x)=\displaystyle\int_{-\infty}^\infty f(x,y)dy\\
&=&\displaystyle\int_{-\infty}^\infty f(x|y)f(y)dy\\
&=&f(x|y)\displaystyle\int_{-\infty}^\infty f(y)dy\\
&=&f(x|y)\\
\implies f(x,y)&=&f(x|y)f_Y(y)=f_X(x)f_Y(y)
\end{array}\]

\theorem{Bayes' Theorem}
Let $X\ \&\ Y$ be \textit{Random Variables}.\\
\textit{Bayes' Theorem} states that
$$f(X|Y)=\dfrac{f(Y|X)x(X)}{f(Y)}$$

\definition{First Order Markov Property}
Let $\textbf{X}:=\{X_1,\dots,X_n\}$ be a set of \textit{Random Variables}.\\
The set $\textbf{X}$ is said to have the \textit{First Order Markov Property} if
$$f(X_i|\textbf{X}_{\neg i})= f(X_i|X_{i-1})\text{ where }\textbf{X}_{\neg i}:=\textbf{X}/\{X_i\}$$
Thus we can infer the \textit{marginal distribution}
$$f(\textbf{X})=f(X_1)\prod_{i=2}^Nf(X_i|X_{i-1})$$

\subsubsection{Probability Distributions}

\definition{$\beta$-Distribution}
Let $X\sim\text{Beta}(\alpha,\beta)$.\\
A \textit{continuous} random variable with shape parameters $\alpha,\beta>0$. Then
\[\begin{array}{rcl}
f_X(x)&\propto&x^{\alpha-1}(1-x)^{\beta-1}\mathds{1}\{x\in[0,1]\}\\
\expect(X)&=&\dfrac{\alpha}{\alpha+\beta}\\
\var(X)&=&\dfrac{\alpha\beta}{(\alpha+\beta)^2(\alpha+\beta+1)}\\
\mathcal{M}_X(t)&=&1+{\displaystyle\sum_{k=1}^\infty\left(\prod_{r=0}^{k-1}\dfrac{\alpha+r}{\alpha+\beta+r}\right)\dfrac{t^k}{k!}}
\end{array}\]

\definition{Bernoulli Distribution}
Let $X\sim\text{Bernoulli}(p)$.\\
A \textit{discrete} random variable which takes 1 with probability $p$ \& 0 with probability $(1-p)$. Then
\[\begin{array}{rcl}
p_X(k)&=&\begin{cases}1-p&\text{if }k=0\\p&\text{if }k=1\\0&\text{otherwise}\end{cases}\\
P_X(k)&=&\begin{cases}0&\text{if }k<0\\1-p&\text{if }k\in[0,1)\\1&\text{otherwise}\end{cases}\\
\expect(X)&=&p\\
\var(X)&=&p(1-p)\\
\mathcal{M}_X(t)&=&(1-p)+pe^t
\end{array}\]
\nb Often we define $q:=1-p$ for simplicity.\\

\definition{Binomial Distribution}
Let $X\sim\text{Binomial}(n,p)$.\\
A \textit{discrete} random variable modelled by a \textit{Binomial Distribution} on $n$ independent events and rate of success $p$.\\
\[\begin{array}{rcl}
p_X(k)&=&{n\choose k}p^k(1-p)^{n-k}\\
P_X(k)&=&\sum_{i=1}^k{n\choose i}p^i(1-p)^{n-i}\\
\expect(X)&=&np\\
\var(X)&=&np(1-p)\\
\mathcal{M}_X(t)&=&[(1-p)+pe^t]^n
\end{array}\]
\nb If $Y:=\sum_{i=1}^nX_i$ where $\X\iid\text{Bernoulli}(p)$ then $Y\sim\text{Binomial}(n,p)$.\\

\definition{Categorical Distribution}
Let $X\sim\text{Categorical}(\textbf{p})$.\\
A \textit{discrete} random varaible where probability vector $\textbf{p}$ for a set of events $\{1,\dots,m\}$.\\
\[\begin{array}{rcl}
f_X(i)=p_i
\end{array}\]

\definition{$\chi^2$ Distribution}
Let $X\sim\chi^2_r$.\\
A \textit{continuous} random variable modelled by the \textit{$\chi^2$ Distribution} with $r$ degrees of freedom. Then
\[\begin{array}{rcl}
f_X(x)&=&\dfrac{1}{2^{r/2}\Gamma(r/2)}x^{\frac{r}{2}-1}e^{-\frac{x}{2}}\\
F_X(x)&=&\dfrac{1}{\Gamma(k/2)}\gamma\left(\frac{r}{2},\frac{x}{2}\right)\\
\expect(X)&=&r\\
\var(X)&=&2r\\
\mathcal{M}_X(t)&=&\indicator\{t<\frac{1}{2}\}(1-2t)^{-\frac{r}{2}}
\end{array}\]
\nb If $Y:=\sum_{i=1}^kZ_i^2$ with $\textbf{Z}\iid\text{Normal}(0,1)$ then $Y\sim\chi^2_k$.\\

\definition{Exponential Distribution}
Let $X\sim\text{Exponential}(\lambda)$.\\
A \textit{continuous} random variable modelled by a \textit{Exponential Distribution} with rate-parameter $\lambda$. Then
\[\begin{array}{rcl}
f_X(x)&=&\indicator\{t\geq0\}.\lambda e^{-\lambda x}\\
F_X(x)&=&\indicator\{t\geq0\}.\left(1-e^{-\lambda x}\right)\\
\expect(X)&=&\dfrac{1}{\lambda}\\
\var(X)&=&\dfrac{1}{\lambda^2}\\
\mathcal{M}_X(t)&=&\indicator\{t<\lambda\}\dfrac{\lambda}{\lambda-t}
\end{array}\]
\nb Exponential Distribution is used to model the wait time between decays of a radioactive source.\\

\definition{Gamma Distribution}
Let $X\sim\Gamma(\alpha,\beta)$.\\
A \textit{continuous} random variable modelled by a \textit{Gamma Distribution} with shape parameter $\alpha>0$ \& rate parameter $\beta$. Then
\[\begin{array}{rcll}
f_X(x)&=&\dfrac{1}{\Gamma(\alpha)}\beta^\alpha x^{\alpha-1}e^{-\beta x}\\
F_X(x)&=&\dfrac{\Gamma(\alpha)}\gamma(\alpha,\beta x)\\
\expect(X)&=&\dfrac{\alpha}{\beta}\\
\var(X)&=&\dfrac{\alpha}{\beta^2}\\
\mathcal{M}_X(t)&=&\indicator\{t<\beta\}\left(1-\frac{t}{\beta}\right)^{-\alpha}
\end{array}\]
\nb There is an equivalent definition of a \textit{Gamma Distribution} in terms of a shape \& \underline{scale} parameter. The scale parameter is 1 over the rate parameter in this definition.\\

\definition{Multinomial Distribution}
Let $\X\sim\text{Multinomial}(n,\textbf{p})$.\\
A \textit{discrete} random varible which models $n$ events with probability vector $\textbf{p}$ for events $\{1,\dots,m\}$.\\
\[\begin{array}{rcl}
f_\X(\x)&=&{\displaystyle\mathds{1}\left\{\sum_{i=1}^mx_i\equiv m\right\}\frac{n!}{x_1!\cdot\dots\cdot x_n!}\prod_{i=1}^np_i^{x_i}}\\
\expect(X_i)&=&np_i\\
\var(X_i)&=&np_i(1-p_i)\\
\cov(X_i,x_j)&=&-np_ip_j\text{ for }i\neq j\\
\mathcal{M}_{X_i}(\theta_i)&=&\left(\displaystyle\sum_{i=1}^mp_ie^{\theta_i}\right)^n
\end{array}\]
\nb In a realisation $\x$ of $\X$, $x_i$ is the number of times event $i$ has occured.\\

\definition{Normal Distribution}
Let $X\sim\text{Normal}(\mu,\sigma^2)$.\\
A \textit{continuous} random variable  with mean $\mu$ \& variance $\sigma^2$.
\[\begin{array}{rcl}
f_X(x)&=&\dfrac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}\\
F_X(x)&=&\dfrac{1}{\sqrt{2\pi\sigma^2}}\int\limits_{-\infty}^xe^{-\frac{(y-\mu)^2}{2\sigma^2}}dy\\
\expect(X)&=&\mu\\
\var(X)&=&\sigma^2\\
\mathcal{M}_X(\theta)&=&e^{\mu\theta+\sigma^2\theta^2(1/2)}
\end{array}\]

\definition{Pareto Distribution}
Let $X\sim\text{Pareto}(x_0,\theta)$.\\
A \textit{continuous} random variable modelled by a \textit{Pareto Distribution} with minimum value $x_0$ \& shape parameter $\alpha>0$. Then
\[\begin{array}{rcll}
f_X(x)&=&\dfrac{\alpha x_0^\alpha}{x^{\alpha+1}}\\
F_X(x)&=&1-\left(\dfrac{x_0}{x}\right)^\alpha\\
\expect(X)&=&\begin{cases}\infty&\alpha\leq1\\\dfrac{\alpha x_0}{\alpha-1}&\alpha>1\end{cases}\\
\var(X)&=&\begin{cases}\infty&\alpha\leq2\\\dfrac{x_0^2\alpha}{(\alpha-1)^2(\alpha-2)}&\alpha>2\end{cases}\\
\mathcal{M}_X(t)&=&\indicator\{t<0\}\alpha(-x_0t)^\alpha\Gamma(-\alpha,-x_0t)
\end{array}\]

\definition{Poisson Distribution}
Let $X\sim\text{Poisson}(\lambda)$.\\
A \textit{discrete} random variable modelled by a \textit{Poisson Distribution} with rate parameter $\lambda$. Then
\[\begin{array}{rcll}
p_X(k)&=&\dfrac{e^{-\lambda}\lambda^k}{k!}&\text{for }k\in\nats_0\\
P_X(k)&=&{\displaystyle e^{-\lambda}\sum_{i=1}^k\frac{\lambda^i}{i!}}\\
\expect(X)&=&\lambda\\
\var(X)&=&\lambda\\
\mathcal{M}_X(t)&=&e^{\lambda(e^t-1)}
\end{array}\]
\nb Poisson Distribution is used to model the number of radioactive decays in a time period.\\

\definition{$t$-Distribution}
Let $X\sim t_r$.\\
A \textit{continuous} random variable with $r$ degrees of freedom. Then
\[\begin{array}{rcll}
f_X(k)&=&{\displaystyle\frac{\Gamma\left(\frac{\nu+1}{2}\right)}{\sqrt{\nu\pi}\Gamma\left(\frac{\nu}{2}\right)}\left(1+\frac{x^2}{\nu}\right)^{-\frac{\nu+1}{2}}}\\
\expect(X)&=&\begin{cases}0&\text{if }\nu>1\\\text{undefined}&\text{otherwise}\end{cases}\\
\var(X)&=&\begin{cases}\frac{\nu}{\nu-2}&\text{if }\nu>2\\\infty&1<\nu\leq2\\\text{undefined}&\text{otherwise}\end{cases}\\
\mathcal{M}_X(t)&=&\text{undefined}
\end{array}\]
\nb Let $Y\sim\text{Normal}(0,1)\ \&\ Z\sim\chi^2_r$ be independent random variables then $X:=\dfrac{Y}{\sqrt{Z/r}}\sim t_r$.\\

\definition{Uniform Distribution - Uniform}
Let $X\sim\text{Uniform}(a,b)$.\\
A \textit{continuous} random variable with lower bound $a$ \& upper bound $b$. Then
\[\begin{array}{rcll}
f_X(x)&=&\begin{cases}\frac{1}{b-a}&x\in[a,b]\\0&\text{otherwise}\end{cases}\\
F_X(x)&=&\begin{cases}0&x<a\\\frac{x-a}{b-a}&x\in[a,b]\\1&\text{otherwise}\end{cases}\\
\expect(X)&=&\frac{1}{2}(a+b)\\
\var(X)&=&\frac{1}{12}(b-a)^2\\
\mathcal{M}_X(t)&=&\begin{cases}\dfrac{e^{tb}-e^{ta}}{t(b-a)}&t\neq0\\1&t=0\end{cases}
\end{array}\]

\end{document}
