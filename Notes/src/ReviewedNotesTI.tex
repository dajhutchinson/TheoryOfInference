\documentclass[11pt,a4paper]{article}

\usepackage[margin=1in, paperwidth=8.3in, paperheight=11.7in]{geometry}
\usepackage[ruled, vlined, linesnumbered]{algorithm2e}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{dsfont}
\usepackage{enumerate}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{changepage} 

\begin{document}

\pagestyle{fancy}
\setlength\parindent{0pt}
\allowdisplaybreaks

\renewcommand{\headrulewidth}{0pt}
\setlist[enumerate,1]{label={\roman*)}}

% Cover page title
\title{Theory of Inference - Reviewed Notes}
\author{Dom Hutchinson}
\date{\today}
\maketitle

% Header
\fancyhead[L]{Dom Hutchinson}
\fancyhead[C]{Theory of Inference - Reviewed Notes}
\fancyhead[R]{\today}

% Counters
\newcounter{definition}[section]
\newcounter{example}[section]
\newcounter{notation}[section]
\newcounter{proposition}[section]
\newcounter{proof}[section]
\newcounter{remark}[section]
\newcounter{theorem}[section]

% commands
\newcommand{\dotprod}[0]{\boldsymbol{\cdot}}
\newcommand{\cosech}[0]{\mathrm{cosech}\ }
\newcommand{\cosec}[0]{\mathrm{cosec}\ }
\newcommand{\sech}[0]{\mathrm{sech}\ }
\newcommand{\prob}[0]{\mathbb{P}}
\newcommand{\nats}[0]{\mathbb{N}}
\newcommand{\cov}[0]{\mathrm{Cov}}
\newcommand{\var}[0]{\mathrm{Var}}
\newcommand{\expect}[0]{\mathbb{E}}
\newcommand{\reals}[0]{\mathbb{R}}
\newcommand{\integers}[0]{\mathbb{Z}}
\newcommand{\indicator}[0]{\mathds{1}}
\newcommand{\nb}[0]{\textit{N.B.} }
\newcommand{\ie}[0]{\textit{i.e.} }
\newcommand{\eg}[0]{\textit{e.g.} }
\newcommand{\X}[0]{\textbf{X}}
\newcommand{\x}[0]{\textbf{x}}
\newcommand{\iid}[0]{\overset{\text{iid}}{\sim}}
\newcommand{\proved}[0]{$\hfill\square$\\}
\newcommand{\argmin}[0]{\text{argmin}}
\newcommand{\argmax}[0]{\text{argmax}}

\newcommand{\definition}[1]{\stepcounter{definition} \textbf{Definition \arabic{section}.\arabic{definition}\ - }\textit{#1}\\}
\newcommand{\definitionn}[1]{\stepcounter{definition} \textbf{Definition \arabic{section}.\arabic{definition}\ - }\textit{#1}}
\newcommand{\proof}[1]{\stepcounter{proof} \textbf{Proof \arabic{section}.\arabic{proof}\ - }\textit{#1}\\}
\newcommand{\prooff}[1]{\stepcounter{proof} \textbf{Proof \arabic{section}.\arabic{proof}\ - }\textit{#1}}
\newcommand{\example}[1]{\stepcounter{example} \textbf{Example \arabic{section}.\arabic{example}\ - }\textit{#1}\\}
\newcommand{\examplee}[1]{\stepcounter{example} \textbf{Example \arabic{section}.\arabic{example}\ - }\textit{#1}}
\newcommand{\notation}[1]{\stepcounter{notation} \textbf{Notation \arabic{section}.\arabic{notation}\ - }\textit{#1}\\}
\newcommand{\notationn}[1]{\stepcounter{notation} \textbf{Notation \arabic{section}.\arabic{notation}\ - }\textit{#1}}
\newcommand{\proposition}[1]{\stepcounter{proposition} \textbf{Proposition \arabic{section}.\arabic{proposition}\ - }\textit{#1}\\}
\newcommand{\propositionn}[1]{\stepcounter{proposition} \textbf{Proposition \arabic{section}.\arabic{proposition}\ - }\textit{#1}}
\newcommand{\remark}[1]{\stepcounter{remark} \textbf{Remark \arabic{section}.\arabic{remark}\ - }\textit{#1}\\}
\newcommand{\remarkk}[1]{\stepcounter{remark} \textbf{Remark \arabic{section}.\arabic{remark}\ - }\textit{#1}}
\newcommand{\theorem}[1]{\stepcounter{theorem} \textbf{Theorem \arabic{section}.\arabic{theorem}\ - }\textit{#1}\\}
\newcommand{\theoremm}[1]{\stepcounter{theorem} \textbf{Theorem \arabic{section}.\arabic{theorem}\ - }\textit{#1}}

\tableofcontents

% Start of content
\newpage

\section{General}

\subsection{Approaches to Inference}

\definition{Statistical Inference}
\textit{Statistical Inference} is the process of taking some data and infering a property of the world from it. This is done by theorising a \textit{Statistical Model} which may have generated the data and then calculating parameters for it from the data.\\

\definition{Statistical Model}
\textit{Statistical Models} are a, simplified, mathematical description for how a set of data could have been generated. In particular, a \textit{Statistical Model} describes the random variability in the data generating process.\\

\definition{Frequentist Inference}
The \textit{Frequentist Approach} to \textit{Statistical Inference} treats model unknowns (paramters or functions) as fixed states of nature whose values we want to estimate.\\
There is no modelling of random variability and thus any that occurs during data collection will be inherited by the model.\\

\remark{Frequentist Inference}
Often in \textit{Frequentist Inference} we use \textit{asymptotic results} which only become exact as the sample size tends to infty. This has practical drawbacks.\\

\definition{Bayesian Inference}
The \textit{Bayesian Approach} to \textit{Statistical Inference} treats unknown model parameters as random variables. We define our initial uncertainty about parameter values (the \textit{Prior Distribution}, $\prob(\Theta)$), observed data is used to update these distributions in order to reach a \text{\textit{Posterior Distribution}}, $\prob(\Theta|X)$.\\
\nb This is done by using \textit{Bayes' Theorem}.\\

\remark{Bayesian Inference}
Often in \textit{Bayesian Inference} we use \textit{simulation methods}, which only become exact as the sample size tends to infty. Again, there are practical drawbacks to this.\\

\remark{Statistical Design}
When trying to infer a model from data there are a few common questions we ask
\begin{enumerate}
	\item What range of parameter values are consistent with the data?
	\item Which of several alternative models could most plausibly have generated the data?
	\item Could our model have generate the data at all?
	\item How coudl we better arrange the data gatehering process to improve the ansers to the preceeding questions?
\end{enumerate}

\subsection{Models}

\definition{Nested Models}
Let $\X_1\sim f_1(\cdot;\pmb\theta_1),\ \X_2\sim f_2(\cdot;\pmb\theta_2)$ for $\pmb\theta_1,\pmb\theta_2\in\pmb\Theta_1$.\\
If $\pmb\theta_1\subset\pmb\theta_2$ then $\X_1$ is \textit{Nested} in $\X_2$.\\

\definition{Predictor Variables}
\textit{Predictor Variables} are the dependent variables of a system, whose values we observe.\\
\nb Typically denoted $\x$ or $\X$.\\

\definition{Metric}
\textit{Metrics} are \textit{Predictor Variables} which measure an explict quantity.\\

\definition{Factor}
\textit{Factors} are \textit{Predictor Variables} which act as labels to whether an observation belongs in a particular class due a property which cannot be explicitly quantified. (\eg Male or Female).\\

\definition{Response Variables}
\textit{Response Variables} are the \underline{in}dependent variables of a system, whose value we observe.\\
\nb Typically denoted $y$ or $\textbf{y}$.\\

\definition{Fitted Values, $\hat{y}$}
\textit{Fitted Values} are our estimated values for the \textit{Response Variable}.
$$\hat{y}_i:=f(\x_i)$$

\definition{Regular Models}
Let $\X\sim f(\cdot;\pmb\theta)$ for $\pmb\theta\in\pmb\Theta$ be a \textit{Statistical Model}.\\
A \textit{Statistical Model} is deemed \textit{Regular} if it fulfils all the following:
\begin{enumerate}
	\item Densities for distinct $\pmb\theta$ are distinct.\\
	\nb If not, parameters won't be identifiable \& thus no guaranteed consistency.
	\item $\pmb\theta^*\in\pmb\Theta$.\\
	\nb Otherwise we cannot approximat \textit{Log-Likelihood} by \textit{Taylor Expansion} in the region of $\pmb\theta^*$.
	\item Within some neighbourhood of $\pmb\theta^*$
	\begin{itemize}
		\item The first three derivatives of the \textit{Log-Likelihood} exist \& are bounded.
		\item $\mathcal{I}:=\expect\left(\dfrac{\partial\ell}{\partial\pmb\theta}\bigg|_{\theta^*}\dfrac{\partial\ell}{\partial\pmb\theta^T}\bigg|_{\theta^*}\right)\equiv-\expect\left(\dfrac{\partial^2\ell}{\partial\pmb\theta\pmb\theta^T}\bigg|_{\theta^*}\right)$ is satisfied.
		\item The \textit{Fisher Information Matrix}, $\mathcal{I}$, is positive-definite \& finite.
	\end{itemize}
\end{enumerate}
\nb These requirements are required for certain results.

\subsection{Graphical Models}
%TODO 10

\definition{Directed Acycle Graphs}
\definition{Stocastic Nodes}
\definition{Deterministic Nodes}

\subsection{Inference by Mathmetical Manipulation}

% Bayesian & Frequentist use computation
% Simulation

\remark{Inference by Mathematical Manipulation}
\textit{Bayesian} and \textit{Frequentist Inference} use mathetmatical computaiton to make inferences about parameter valus \& their uncertainty. An alternative approach is to use mathematical manipulation, rather than computation.\\

\definition{Bootstrapping}
Let $X$ be a set of observed data.\\
\textit{Bootstrapping} is a \textit{simulation} of the data gathering process.\\
In \textit{Bootstrapping} we uniformly sample values from $X$ \underline{with replacement} until we reach a desired sample size (often $|X|$).\\

\proposition{Inference by Resampling}
Once we have used \textit{Bootstrapping} to generate a set of new data-sets we can use \textit{Bayesian} \& \textit{Frequentist Inference} techniques in order to estimate parameter values.\\

\remark{Bootstrap Interval}
An \textit{Interval} generated from \textit{Bootstrapping} datasets are generally narrower than those produced by \textit{Bayesian} or \textit{Frequentist Approaches}.\\
\nb This discrepancy is reduced as sample size increases.\\

\proposition{Bootstrap Percentiles}
When wishing to create an interval for $\theta$ using \textit{Bootstrapped} data, we treat the $\hat\theta$ values as if they came from $\prob(\mu|X)$.

\subsection{Causality}

\definition{Causality}
\textit{Causality} is a relationship between two events where one of the events caused the other.\\

\definition{Correlation}
\textit{Causality} is a relationship between two events where the events are likely to occur together. This does not mean that one event has caused the other, but they may have been caused by the same variable (which may be hidden).\\

\remark{When two variables are highly correlated it is hard to distinguish their effects}
 
\definition{Counfounding Variable}
A \textit{Confounding Variable} is a variable which influences both the \textit{Predictor} \& \textit{Response Variables} in a system.\\
Suppose $x,h$ are highly-correlated and $y=\beta_0+\beta_1x$. The model $y=\beta_2+\beta_3h$ would appear statistically good even though $h$ had no part in geneating $y$. Here $x$ is the \textit{Confounding Variable}.\\
\nb AKA \textit{Hidden Variables}

\subsubsection{Controlled Experiments}

\definition{Randomisation}
\textit{Randomisatiation} is a technique used when designing experiments to break relationships between \textit{Confounder Variables} and our \textit{Response Variable}.\\
\textit{Randomisation} involves taking a set of subjects and randomly assigning them to different ``treatments".\\
Provided these treaments only vary the \textit{Predictor Variable} we wish to test, this breaks association between other \textit{Prectior Variables} \& our \textit{Response Variable}. Making these \textit{Predictor Variables} now part of the ranom variability of the model, $\varepsilon$.\\
 
\remark{Randomisation is the Gold Standard for Inferring Causation}

\proposition{Mathematical Justification}
Consider model matrix $(\X,\textbf{H})$ where $\X$ is formed from observed \textit{Predictor Variables} \& $\textbf{H}$ is from \textit{Confounding Variables} (\nb we would not know its value in practice).\\
Assume the oolumns of $\textbf{H}$ are centred on 0.\\
We now have \textit{Least Squares Estimate} for the parameters of
$$\begin{pmatrix}\tilde{\pmb\beta}_X\\\tilde{\pmb\beta}_H\end{pmatrix}=\begin{pmatrix}\X^T\X&\X^T\textbf{H}\\\textbf{H}^T\X&\textbf{H}^T\textbf{H}\end{pmatrix}^{-1}\begin{pmatrix}\X^T\\\textbf{H}^T\end{pmatrix}\textbf{y}$$
If $\X,\textbf{H}$ are dependent on each other then $\X^T\textbf{H}\neq\pmb0\implies\tilde{\pmb\beta}_X\neq(\X^T\X)^{-1}\X^T\textbf{y}=\hat{\pmb\beta}_X$.\\
If $\X,\textbf{H}$ are \underline{independent} of each other then $\X^T\textbf{H}=\pmb0$, for a large sample size,\\
$\implies\tilde{\pmb\beta}_X=(\X^T\X)^{-1}\X^T\textbf{y}=\hat{\pmb\beta}_X$.\\
\nb \textit{Randomisation} ensures $\X$ and $\textbf{H}$ are independent.

\subsubsection{Instrumental Variables}

\remark{Motivation}
In some scenarios it is impractical or unethical to perform randomised experiments.\\

\definition{Instrumental Variables}
Let $(\X,\textbf{H})$ be a \textit{Model Matrix} with $\X$ observed \& $\textbf{H}$ confounding for our \textit{Response Variable}, $\textbf{y}$.\\
A variable is an \textit{Instrumental Variable} if
\begin{itemize}
	\item It is \underline{not} part of the true model of the \textbf{Response Variable};
	\item It \underline{is} correlated with the \textit{Predictor Variables} in $\X$;
	\item[And,] It is \underline{not} correlated with the \textit{Confounding Variables} in $\textbf{H}$.
\end{itemize}

\remark{Finding Instrumental Variables is Hard, very!}

\proof{Confounding Variables affect Least Squares Estimate in Linear Models}
Let $(\X,\textbf{H})$ be a \textit{Model Matrix} where $\X$ comes from some observed variables \& $\textbf{H}$ is from \textit{Confounding Variables}.\\
This means the true model is of the form
$$\textbf{y}=\X\pmb\beta_X+\textbf{H}\pmb\beta_H+\pmb\varepsilon$$
Since we only know $\X$ we try to fit $\textbf{y}=\X\pmb\beta_X+\textbf{e}$.\\
Effectively $\textbf{e}=\textbf{H}\pmb\beta_H+\pmb\varepsilon$, which is unlikey to fulfil the assumptions of indepdence \& constant variance.\\
This is a problem for the model. Further
\[\begin{array}{rcl}
\expect(\hat{\pmb\beta}_X)&=&(\X^T\X)^{-1}\X^T(\X,\textbf{H})\pmb\beta\\
&=&(\X^T\X)^{-1}\X^T\X\pmb\beta_X+(\X^T\X)^{-1}\X^T\textbf{H}\pmb\beta_H\\
&=&\pmb\beta_X+(\X^T\X)^{-1}\X^T\textbf{H}\pmb\beta_H\\
&\neq&\pmb\beta_X
\end{array}\]
\nb The space spanned by the columns of $\X$ is not orthogonal to $\textbf{e}$.\\

\proposition{Instrumental Variables for Linear Models}
Let $(\X,\textbf{H})$ be a \textit{Model Matrix} where $\X$ comes from some observed variables \& $\textbf{H}$ is from \textit{Confounding Variables}.\\
Let $\textbf{Z}$ be the \textit{Model Matrix} for some \textit{Instrumental Variables}.\\
We assume $\text{Rank}(\textbf{Z})\geq\text{Rank}(\X)$.\\
Perform the projection of $\X$ onto the column space of $\textbf{Z}$. Giving
$$\X_Z:=\textbf{Z}(\textbf{Z}^T\textbf{Z})^{-1}\textbf{Z}^T\X=\textbf{A}_Z\X$$
This gives us the least squares estimate of the model parameters
$$\hat{\pmb\beta}_X=(\X_Z^T\X_Z)^{-1}\X_Z^T\textbf{y}=(\X^T\textbf{A}_Z\X)^{-1}\X^T\textbf{A}_Z\textbf{y}$$
Since $\textbf{Z}$ is independent of $\textbf{H}$. $\textbf{Z}^T\textbf{H}\simeq\pmb0\implies\textbf{A}_Z\textbf{H}\simeq\pmb0$. Thus
\[\begin{array}{rcl}
\expect(\hat{\pmb\beta}_X)&=&(\X^T\textbf{A}_Z\X)^{-1}\X^TA_Z(\X,\textbf{H})\pmb\beta\\
&=&(\X\textbf{A}_Z\X)^{-1}\X^T\textbf{A}_Z\X\pmb\beta_X+\underbrace{(\X\textbf{A}_Z\X)^{-1}\X^T\underbrace{\textbf{A}_Z}_{\simeq0}\textbf{H}\pmb\beta_H}_{\simeq0}\\
&=&(\X\textbf{A}_Z\X)^{-1}\X^T\textbf{A}_Z\X\pmb\beta_X\\
&=&\pmb\beta_X
\end{array}\]
This shows the estimate produced by using \textit{Instrumental Variables} produces a much more accurate estimate of the model parameters, than when they are not used \& \textit{Convolution Variables} exist.

\subsection{Hypothesis Testing}

\remark{Hypothesis Testing is \underline{mostly} a Frequentist method, not Bayesian.}

\remark{Hyphothesis Testing amounts to choosing the simplist model which has no obvious incosistencies with the data}

\definition{Simple Hypothesis}
A \textit{Simple Hypothesis} states that a parameter takes an \underline{exact} value.\\
\ie $\theta=\theta_0$ for $\theta_0\in\Theta$.\\

\definition{Composite Hypothesis}
A \textit{Composite Hypothesis} states that a parameter takes a value from a set.\\
\ie $\theta\in\Theta_0$ for $\Theta_0\subseteq\Theta$.\\

\definition{Test Statistic}
A \textit{Test Statistic} is a random variable whose value depends on the observed set of data.\\
\textit{Test Statistics} are used to assess the likelihood of observing a certain data set under a given \textit{Null Hypothesis} in \textit{Hypothesis Testing}.\\

\definition{$p$-Value}
\textit{$p$-Value} measures the goodness of fit of statistical models.\\
The \textit{$p$-Value} is the probability of observing more extreme that the data used for fitting, under the theorised model, $\pmb\Theta_0$.\\
Let $\X\sim f(\cdot;\pmb\theta)$ and $\x$ be a realisation.
$$p(\x):=\sup_{\theta\in\Theta_0}\prob(\X\geq \x;\pmb\theta)=\sup_{\theta\in\Theta_0}f(\textbf{x};\pmb\theta)$$
A higher \textit{$p$-Value} suggests a better model fit.\\

\remark{$p$-Values are good at testing the fit of a model, but not at comparing the relative fit of two models}

\remark{$p(\x)$ is the smallest Significance Level at which we would reject the \text{Null Hypothesis}}

\definition{Hypothesis Testing}
\textit{Hypothesis Testing} is the porcess of determining which of two hypotheses about model parameters is more consistent with the data.\\
We define a \textit{Null Hypothesis} and an \textit{Alternative Hypothesis}. The \textit{Null Hypothesis} acts as our default position, and we only reject it is if the observed data is too extreme (given that it is true).\\
\nb \textit{Null} and \textit{Alternative Hypothesis} are mutually exclusive.\\

\proposition{Process for Hypothesis Testing}
Let $\x$ be a realisation of $\X$
\begin{enumerate}
	\item Choose a model $f(\cdot;\theta)$ st $\X\sim f(\cdot;\theta)$ for $\theta\in\Theta$.
	\item Define a \textit{Null Hypothesis}, $H_0$, and an \textit{Alternative Hypothesis}, $H_1$.
	\item Define a \textit{Test Statistic}, $T(\cdot)$.
	\item Choose a \textit{Significance Level}, $\alpha$, and calculate the equivalent \textit{Critical Value}, $c$, for the \textit{Test Statistic}.
	\item Calculate value of the \textit{Test Statistic} under the observed data, $t_\text{obs}=T(\x)$.
	\item If $t_\text{obs}\geq c$ then reject $H_0$ in favour of $H_1$, otherwise accept $H_0$.
\end{enumerate}

\definition{Neymann-Pearson Test Statistic}
The \textit{Neymann-Pearson Test Statistic} is a generalisation of the \textit{Likelihood Ratio}.\\
It measures the likelihood of the \textit{Alternative Hypothesis} being correct, relative to the \textit{Null Hypothesis}, given the data.\\
Let $\X\sim f(\cdot;\pmb\theta)$, $\x$ be a realisation of $\X$ and $H_0:\pmb\theta\in\pmb\Theta_0$ be our \textit{Null Hypothesis}.
$$T_{np}(\x):=\dfrac{p(\x;\pmb\Theta)}{p(\x;\pmb\Theta_0)}=\dfrac{\sup_{\theta\in\Theta} f(\x;\pmb\theta)}{\sup_{\theta_0\in\Theta_0}f(\x;\pmb\theta_0)}=\dfrac{f(\x;\hat{\pmb\theta}_\text{MLE})}{\sup_{\theta_0\in\Theta_0}f(\x;\pmb\theta_0)}\geq1$$
Lower values of the \textit{Likelihood Ratio} indicate that $H_0$ is more likely to be true.\\

\definition{Power Function, $\pi$}
The \textit{Power Function}, $\pi(\cdot)$, measures the probability of rejecting the \textit{Null-Hypothesis} given that another set of parameter values is true (usually test with the \textit{Alternative Hypothesis}).\\
Let $\X\sim f(\cdot;\theta)$, $T(\cdot)$ be a \textit{Test Statistic} and $c$ be the \textit{Critical Value} of $T$. Then
$$\pi(\theta_1;T,c)=\prob(T(\X)\geq c;\theta_1)$$

\theorem{Neyman-Pearson Lemma}
Let $\X\sim f(\cdot;\pmb\theta)$ and $\x$ be a realisation of $\x$.\\
Consider testing two \textit{Simple Hypothese} with the \textit{Neyman Pearson Test Statistic}.\\
Choose some $\alpha\in[0,1]$ and find $c_{NP}$ st $\alpha=\prob(T_{NP}\geq c_{NP};\pmb\theta_0)$.\\
Then the test $(T_{NP},c_{NP})$ is equivalent to the uniformly most powerful test.\\
%TODO proof

\definition{Generalised Likelihood Ratio Test Statistic}
Let $\X\sim f(\cdot;\pmb\theta)$ be a model \& $\x$ be a realisation of $\X$.\\
Consider testing two, \underline{non-nested}, hypotheses
$$H_0:\textbf{R}(\pmb\theta)=\pmb0\quad\text{against}\quad H_1:\textbf{R}(\pmb\theta)neq\pmb0$$
where $\textbf{R}(\cdot)$ is a vector-valued function of $\pmb\theta$ st $H_0$ imposes $r$ restrictions on $\pmb\Theta$.\\
The \textit{Generalised Likelihood Ratio Test Statistic} is defined as
$$T_{GLR}:=2[\ell(\hat{\pmb\theta}_\text{MLE})-\sup_{\textbf{R}(\pmb\theta)=\pmb0}\ell(\pmb\theta)]\sim\chi^2_r$$
%TODO proof

\subsection{Intervals}

\definition{Random Interval}
Let $\X\sim f_n(\cdot;\theta^*)$ for $\theta^*\in\Theta$ and $L,U:\mathcal{X}^n\to\Theta$ st $\forall\ \x\in\mathcal{X}^n\ L(\x)<U(\x)$.\\
A \textit{Random Interval} is an \textit{Interval} whose bounds depends on a \textit{Random Variable}.\\
Here $\mathcal{I}(\X):=[L(\X),U(\X)]$ is a \textit{Random Interval}.\\
\nb $L(\cdot)$ \& $U(\cdot)$ are maps from observed data to parameter values.\\

\definition{Coverage of an Interval}
Let $\mathcal{I}(\X):=[L(\X),U(\X)]$ be a \textit{Random Interval} for $\theta$ with true value $\theta^*$.\\
The \textit{Coverage of an Interval} is the probability that the true value of the parameter it is estimating lies in the inverval.
$$C_\mathcal{I}=\prob(\theta^*\in\mathcal{I}(\X);\theta^*)$$

\definition{Confidence Interval}
A $1-\alpha$ \textit{Confidence Interval} for a parameter is an interval with \textit{Coverage} at least $1-\alpha$.
\begin{center}
$\mathcal{I}(\X):=[L(\X),U(\X)]$ is a $1-\alpha$ \textit{Confidence Interval} if $\prob(\theta^*\in\mathcal{I})\geq1-\alpha$
\end{center}
\nb If $\prob(\theta^*\in\mathcal{I}(\X))=1-\alpha$ then $\mathcal{I}$ is an \textit{\underline{Exact} Confidence Interval}.\\

\remark{Confidence Intervals are a part of Frequentist Statistics, not Bayesian}

\definition{Credible Interval}
Let $\X\sim f(\cdot;\pmb\theta)$ and $\x$ be a realisation of $\X$.\\
A $1-\alpha$ \textit{Credible Interval} is as an interval $(\theta_1,\theta_2)$ where the probability that the true parameter lies in the parameter is $1-\alpha$.
$$1-\alpha=\prob(\theta^*\in(\theta_1,\theta_2)\Longleftrightarrow\int^{\theta_1}_{\theta_2}p(\theta|\theta)d\theta=1-\alpha$$
When the model has multiple parameters we find a different intervval for each parameter.\\
\nb Multiple such intervals will exist and we find these intervals by sampling.\\

\remark{Credible Intervals are a part of Bayesian Statistics, not Frequentist}

\definition{Test Inversion}
\textit{Test Inversion} is the process of finding a \textit{Confidence Interval/Set} by finding the range of values which would cause the \textit{Null Hypothesis} to be accepted.\\
\nb AKA \textit{Wilk's Intervals}.\\

\propositionn{\text{Every value in a Wilk's Interval has a greater likelihood than any value outside.}}


\subsection{Other ways of Assessing Fit}

\definition{Residual}
The \textit{Residual} is the difference between the true value of the \textit{Response Variables} \& our \textit{Fitted Values}.
$$\epsilon:=|y_i-\hat{y}_i|$$

\definition{Residual Sum of Squares}
The \textit{Residual Sum of Squares} is the sum of the squared value of the \textit{Residuals} for each observation.\\
The \textit{RSS} is used as a measure for how well our model fits the data
$$RSS:=\sum_{i=1}^N\epsilon_i^2=\sum_{i=1}^N(y_i-\hat{y}_i)^2=\|\textbf{y}-\hat{\textbf{y}}\|^2$$

\remark{More complicated models tend to have greater likelihood}
whether or not the extra complications reflect anything in the true data generating process.\\

\definition{Kullback-Leibler Divergence}
\textit{Kullback-Leibler Divergence} is a measure of how similar two models are.\\
Let $f(\cdot)$ be the true model \& $g(\cdot)$ be an approximation.
$$KL(f,g)=\int[\ln f(\textbf{y})-\ln g(\textbf{y})]f(\textbf{y})d\textbf{y}$$
\nb This is a measure of information loss caused by $g(\cdot)$.\\

\proposition{Testing Models with KL Divergence}
Let $f$ be a proposed model with true parameters values $\pmb\theta^*$.\\
To compare a proposed set of parameters, $\pmb\theta$, we want to calculate
$$KL(f^*,f_\theta)=\int[\ln f(\textbf{y};\pmb\theta^*)-\ln f(\textbf{y};\pmb\theta)]f(\textbf{y};\pmb\theta^*)d\textbf{y}$$
\nb This is not tractable since $f^*$ is unknown (otherwise we would use it).\\

\definition{Akaike's Information Criterion}
\textit{Akaike's Information Criterion}, AIC, is a goodness-of-fit measure for models.
$$AIC(\pmb\theta)=-2\ell(\pmb\theta)+p$$
where $p:=|\pmb\theta|$ (the number of estimated parameters).\\
Lower \textit{AIC} values indicate better fit.\\
\nb The factor of $2$ is to put \textit{AIC} on the scale as the $T_{GLR}$.\\

\remark{AIC is non-consistent.}
As $n\to\infty$ the probability of choosing the correct model does not tend to 1.\\

\proposition{Using KL Divergence}
Let $f$ be a proposed model with true parameters $\pmb\theta^*$.\\
We want to find the set of parameters, $\hat{\pmb\theta}$, which perform closely to the true values.\\
\ie $\argmin_{\hat{\pmb\theta}}KL(f_{\hat\theta},f^*)$ where $f_{\hat\theta}:=f(\cdot;\hat{\pmb\theta})$ \& $f^*:=f(\cdot;\pmb\theta^*)$.\\
It turns out that $\expect(KL(f_{\hat\theta_\text{MLE}},f^*))$ is tractable.
$$\hat\expect(KL(f_{\hat\theta_\text{MLE}},f^*))=-\ell(\pmb\theta_\text{MLE})+p+\int\ell(\pmb\theta^*;\textbf{y})f(\textbf{y};\pmb\theta^*)d\textbf{y}$$
Where $p:=|\pmb\theta|$ (the number of estimaed parameters).\\
\text{Note that this involves truth, thus is minised by whichever model minimises \textit{Akaike's Information Criterion}.}
%TODO derivation (done on paper)



\section{Linear Models}

\proposition{Implementing Factors}
Suppose a model has \textit{Factor Variable} $g_i$ which seperates observations into $n$ categories.\\
In the function for $y_i$, $g_i$ would be represented by a single term $\gamma_{g_i}$ whose value depends on the value of $g_i$. (\ie A different weight is assigned to each group). \\
We want to find the $n$ values which $\gamma_{g_i}$ can take.\\
We can express this in terms of matrices, as below, with each row on the LHS denoting which category each observation belongs to
$$\begin{pmatrix}0&1&0&\dots&\\1&0&0&\dots\\0&0&1&\dots\\\vdots&\vdots&\vdots&\ddots\end{pmatrix}\begin{pmatrix}\gamma_1\\\gamma_2\vdots\\\gamma_n\end{pmatrix}$$
\nb Each row has a single $1$ element and $n-1$ zero elements.\\

\remark{We want to find the parameters to the Precitor Variables which produce accurate values for the Response Variables.}

\definition{Model Matrix}
Each element in a \textit{Model Matrix} is a function of the \textit{Predictor Variables}.\\
Each row depends on a different set of observations.\\

\definition{Linear Model}
A \textit{Linear Model} is a \textit{Statistical Model} whose response vector, $\textbf{y}$, is linear wrt its \textit{Model Matrix}, $\X$, and some zero-mean random error, $\pmb\varepsilon$.
$$\textbf{y}=\X\pmb\beta+\pmb\varepsilon$$
$\X\in\reals^{n\times p},\ \pmb\beta\in\reals^p$ and $\varepsilon\sim\text{Normal}(\pmb0,\sigma^2 I)$.

\subsection{Frequentist Approach}

\proposition{Frequentist Approach to Linear Models}
In the \textit{Frequentist Approach} to \textit{Linear Models} we treat $\pmb\beta$ and $\sigma^2$ as fixed (but unknown) states of nature.\\
Thus all random variability from the data will be inherited into the model.\\

\subsubsection{Estimation}

\proposition{Point Value Estimates}
We can make \textit{Point Value Estimates} of parameter values by finding the set of parameters $\pmb\beta$ which minimises the \textit{Residual Sum of Squares}.
\[\begin{array}{rrl}
\hat{\pmb\beta}_\text{LSE}&:=&\displaystyle{\argmin_{\pmb\beta}\sum_{i=1}^N(y_i-(\X\pmb\beta)_i)^2}\\
&=&\displaystyle{\argmin_{\pmb\beta}\|\textbf{y}-\X\pmb\beta\|^2}
\end{array}\]
\nb This is the \textit{Least Squares Estimate} of $\pmb\beta$.\\

\remark{$\hat{\pmb\beta}_\text{LSE}=R^{-1}Q^T\textbf{y}$}
where $Q,R$ are from the decomposition of $\X$ st $\X=QR$ with $Q$ being \textit{Orthogonal} and $R$ being \textit{Upper-Triangle}.\\

\proposition{Deriving Least Squares Estimate for $\pmb\beta$}
Let $\X,\textbf{y}$ be $n$ observed data points \& $\pmb\beta\in\reals^p$ be a parameter vector we are fitting to our model.\\
We want to find $\hat{\pmb\beta}_\text{LSE}=\displaystyle{\argmin_{\pmb\beta}\|\textbf{y}-\X\pmb\beta\|^2}$.\\
Since $\X$ is a real-valued matrix it can be decomposed into
$$\X=\mathcal{Q}\begin{pmatrix}R\\\pmb0\end{pmatrix}=QR\footnote{Known as \textit{QR Decomposition} and can be performed in R using $\mathtt{qr.Q(qr(X),complete=TRUE)}\ \&\ \mathtt{qr.R(qr(X))}$}$$
where $R\in\reals^{p\times p}$ is an upper triangle matrix, $\mathcal{Q}\in\reals^{n\times n}$ is orthogonal \& $Q\in\reals^{n\times p}$ is the first $p$ columns of $\mathcal{Q}$.\\
Note that $\mathcal{Q}^T$ is \textit{Orthogonal}.\\
Thus
\[\begin{array}{rcl}
\|\textbf{y}-\X\pmb\beta\|^2&=&\left\|\textbf{y}-\mathcal{Q}\begin{pmatrix}R\\\pmb0\end{pmatrix}\pmb\beta\right\|^2\\
&=&\left\|\mathcal{Q}^T\textbf{y}-\mathcal{Q}^T\mathcal{Q}\begin{pmatrix}R\\\pmb0\end{pmatrix}\pmb\beta\right\|^2\text{ since }\mathcal{Q}^T\text{ is orthogonal}\\
&=&\left\|\mathcal{Q}^T\textbf{y}-\begin{pmatrix}R\\\pmb0\end{pmatrix}\pmb\beta\right\|^2
\end{array}\]
Decompose $\mathcal{Q}^T\textbf{y}=\begin{pmatrix}\textbf{f}\\\textbf{r}\end{pmatrix}$ with $\textbf{f}\in\reals^p$ \& $\textbf{r}\in\reals^{n-p}$.\\
Note that $\textbf{f}=Q^T\textbf{y}$.\\
$\textbf{f}$ is the first $p$ rows of $\mathcal{Q}^T\textbf{y}$ and $\textbf{r}$ is the last $n-p$ rows.\\
Thus
\[\begin{array}{rcl}
\|\textbf{y}-\X\pmb\beta\|^2&=&\left\|\begin{pmatrix}\textbf{f}\\\textbf{r}\end{pmatrix}-\begin{pmatrix}R\\\pmb0\end{pmatrix}\pmb\beta\right\|^2\\
&=&\|\textbf{f}-R\pmb\beta\|^2+\|\textbf{r}\|^2
\end{array}\]
$\|\textbf{r}\|^2$ is indepdent of $\pmb\beta$ and thus irreducible.\\
This final expression is minimised when $\|\textbf{f}-R\pmb\beta\|^2=0$ (Meaning $\|\textbf{r}\|^2=\|\textbf{y}-\X\pmb\beta\|^2$).\\
Thus
$$\hat{\pmb\beta}_\text{LSE}=R^{-1}\textbf{f}=R^{-1}Q^T\textbf{y}$$
This requires that $R$ is full rank, in order for its inverse to exist.\\
Further, $\X$ has to have full rank, which we can ensure by our design of the model.\\

\proposition{Least Squares Estimate of Parameter Vector is Unbiased}
$$\expect(\hat{\pmb\beta})=\expect(R^{-1}Q^T\textbf{y})=R^{-1}Q^T\expect(\textbf{y})=R^{-1}Q^T\X\pmb\beta=R^{-1}Q^TQR\pmb\beta=\pmb\beta$$

\propositionn{Variance of Least Squares Estimate of Parameter Vector}
\[\begin{array}{rrcl}
&\cov(\textbf{y})&=&I\sigma^2\\
\implies&\cov(\textbf{f})&=&Q^T\textbf{y}\\
&&=&Q^TQ\sigma^2\\
&&=&I\sigma^2\\
\implies&\cov(\hat{\pmb\beta}_\text{LSE})&=&\cov(R^{-1}\textbf{f})\\
&&=&R^{-1}\cov(\textbf{f})R^{-T}\\
&&=&R^{-1}I\sigma^2R^{-T}\\
&&=&R^{-1}R^{-T}\sigma^2
\end{array}\]

\remark{Least Squares Estimation - Geometric Interpretation}
\textit{Linear Models} state that $\expect(\textbf{y})$ lies on the space spanned by all possible linear combinations of the columns of the \textit{Model Matrix}.\\
\textit{Least Squares Estimation} finds the point in the space closest to $\textbf{y}$.\\
Thus \textit{Least Sqaures Estimation} amounts to find the orthogonal projection of $\textbf{y}$ in the linear space spanned by the columns of $\X$.\\

\definition{Influence Matrix}
The \textit{Influence Matrix}, $A$, is the orthogonal projection of the response variables onto the linear space spanned by the columns of $\X$.\\
Since
$$\hat{\textbf{y}}=\X\hat{\pmb\beta}=(QR)(R^{-1}Q^T\textbf{y})=QQ^T\textbf{y}$$
the \textit{Influence Matrix} is
$$A=QQ^T$$
\nb The \textit{Influence Matrix} is \textit{Idempotent}, $AA=A$.\\

\propositionn{Results in terms of Model Matrix}
\[\begin{array}{rcl}
\hat{\pmb\beta}&=&(\X^T\X)^{-1}\X^T\textbf{y}\\
\Sigma_{\hat{\pmb\beta}}&=&(\X^T\X)^{-1}\sigma^2\\
A&=&\X(\X^T\X)^{-1}\X^T
\end{array}\]
\nb Substituting $\X=QR$ gets back to previous results.

\subsubsection{Checking}
%Frequentist

\remark{Assumptions}
We assume that each $\varepsilon_i$ is independent \& has constant variance (we also assume they are normally distributed but this generaly holds due to CLT).\\
We need a way to check this assumption holds in order for inferences (beyound point estimates) to be sound.\\

\proposition{Graphical Checks}
Plotting $\hat\epsilon=y_i-(\X\hat{\pmb\beta})_i$ on a graph tends to indicate whether an assumption has been broken, and if so, how it was broken.
\begin{itemize}
	\item[-] Systematic patterns in the mean indicate independence assumption is broken.
	\item[-] Systematic patterns in the variability indicate the constant variance assumption is broken.
\end{itemize}

\subsubsection{Evaluating}

\remark{Choice of measure to minimise?}
Was choosing to minimise \textit{Residual Sum of Squares} a good one?\\
\nb Choosing $\sum_i|\epsilon_i|$, $\sum_i\epsilon^4$,\dots could have worked.\\

\remark{Problem with $\|\hat{\pmb\beta}-\pmb\beta\|$ as measure}
Suppose our data has a lot of information for estimating $\beta_i$ but not mch for $\beta_j$, should we weight them equally?\\

\remark{Preferred Estimators}
We require estiamtors to be \textit{Unbiased}, and then we shall choose the estimator with the least variance among those which are \textit{Unbiased}.\\
\nb Least variance means smallest covariance matrix (in a way which accounts for weighting individual parameters).\\

\theorem{Gauss Markov Theorem}
Let $\X,\textbf{y}$ be some observed data.\\
Consider a model where $\pmb\mu:=\expect(\textbf{y})=\X\pmb\beta$ and $\Sigma^2_y=\sigma^2I$.\\
Let $\tilde\theta:=\textbf{c}^T\textbf{y}$ be any \textit{Unbiased Linear Estimaor} of $\theta=\textbf{t}^T\pmb\beta$ for some arbitrary vector, $\textbf{t}$.\\
Then
$$\var(\tilde\theta)\geq\var(\hat\theta)$$
where $\hat\theta=\textbf{t}^T\hat{\pmb\beta}$ and $\hat{\pmb\beta}=R^{-1}Q^T\textbf{y}$ where $\X=QR$.\\
Thus each element of $\hat{\pmb\beta}$ is a \textit{minimum variance unbiased estimator}, since $\textbf{t}$ is arbitrary.

\subsubsection{Hypothesis Testing \& Intervals}

\remark{Populat Hypothesis Test}
Often we want to test whether any $\beta_i=0$ as this would indicate that those predictors do not affect the model accuracy.\\

\proposition{Distribution of $\hat{\pmb\beta}$}
We assume that $\varepsilon_i\sim\text{Normal}(0,\sigma^2)$. Thus
\[\begin{array}{rcl}
\textbf{y}&\sim&\text{Normal}(\X\pmb\beta,\sigma^2I)\\
\implies\hat{\pmb\beta}&\sim&\text{Normal}(\pmb\beta,R^{-1}R^{-T}\sigma^2)
\end{array}\]
Note that $\pmb\beta$ and $\sigma^2$ are unknown.\\
\nb $\X=QR$ where $Q$ is orthogonal \& $R$ upper-triangle.

\proposition{$\dfrac{\hat\beta_i-\beta_i}{\hat{\sigma}_{{\hat\beta}_i}}\sim t_{n-p}$}
Note that we can produce a decomposition $\X=\mathcal{Q}\begin{pmatrix}R\\\pmb0\end{pmatrix}$ where $\mathcal{Q}$ is orthogonal \& $R$ is upper triangular.\\
We have
$$\cov(\mathcal{Q}^T\textbf{y})=\mathcal{Q}^T\cov(\textbf{y})\mathcal{Q}^{-T}=\mathcal{Q}^T\cov(\textbf{y})\mathcal{Q}=\mathcal{Q}^TI\sigma^2\mathcal{Q}=I\sigma^2$$
This implies that elements of $\mathcal{Q}^T\textbf{y}$ are independent, due to their assumed normal distribution.\\
Note that
$$\expect(\mathcal{Q}^T\textbf{y})=\expect\left(\begin{pmatrix}\textbf{f}\\\textbf{r}\end{pmatrix}\right)\quad\text{and}\quad\expect(\mathcal{Q}^T\textbf{y})=\mathcal{Q}^T\expect(\textbf{y})=\mathcal{Q}^T\X\pmb\beta=\begin{pmatrix}R\\\pmb0\end{pmatrix}\pmb\beta$$
Thus
$$\expect\left(\begin{pmatrix}\textbf{f}\\\textbf{r}\end{pmatrix}\right)=\begin{pmatrix}R\\\pmb0\end{pmatrix}\pmb\beta\implies\expect(\textbf{f})=R\pmb\beta\ \&\ \expect(\textbf{r})=\pmb0$$
Further
$$\textbf{f}\sim\text{Normal}(R\pmb\beta,I_p\sigma^2)\quad\text{and}\quad\textbf{r}\sim\text{Normal}(\pmb0,I_{n-p}\sigma^2)$$
and $\textbf{f}$ \& $\textbf{r}$ are independent.\\
Thus $\hat{\pmb\beta}$ \& $\hat\sigma^2$ are independent.\\
Since each $r_i\sim\text{Normal}(0,\sigma^2)$
$$\frac{\|\textbf{r}\|^2}{\sigma^2}=\frac{1}{\sigma^2}\sum_{i=1}^{n-p}r_i^2\sim\chi^2_{n-p}$$
Since $\expect(\chi^2_{n-p})=n-p\implies\hat\sigma^2:=\frac{1}{n-p}\|\textbf{r}\|^2$ is an unbiased estimator of $\sigma^2$.\\
$\hat\Sigma_{\hat{\pmb\beta}}:=\Sigma_{\hat{\pmb\beta}}\frac{\hat\sigma^2}{\sigma^2}=R^{-1}R^{-T}\hat\sigma^2$ is an unbiased etimator of $\Sigma_{\hat{\pmb\beta}}$.\\
Thus $\hat\sigma_{\hat\beta_i}:=\sqrt{[\hat\Sigma_{\hat\beta}]_i}=\sigma_{\hat\beta_i}\frac{\hat\sigma}\sigma$.\\
Finally
$$\dfrac{\hat\beta_i-\beta_i}{\hat\sigma_{\hat\beta_i}}=\dfrac{\hat\beta_i-\beta_i}{\sigma_{\hat\beta_i}\frac{\hat\sigma}{\sigma}}=\frac{\frac1{\sigma_{\hat\beta_i}}(\hat\beta_i-\beta_i)}{\sqrt{\hat\sigma^2/\sigma^2}}=\frac{\frac1{\sigma_{\hat\beta_i}}(\hat\beta_i-\beta_i)}{\sqrt{\frac1{\sigma^2}\frac1{n-p}\|\textbf{r}\|^2}}\sim\frac{\text{Normal}(0,1)}{\sqrt{\frac{1}{n-p}\chi^2_{n-p}}}\sim t_{n-p}$$
\nb $\|\textbf{r}\|^2=\|\textbf{y}-\X\hat{\pmb\beta}\|^2$ by the results in \textbf{Proposition 2.4}.\\

\proposition{Confidence Interval for $\beta_i$}
Using the result in \textbf{Proposition 2.9} we can construct the following $1-\alpha$ confidence interval
$$\prob\left(-t_{n-p,\frac{\alpha}{2}}<\dfrac{\hat\beta_i-\beta_i}{\hat{\sigma}_{{\hat\beta}_i}}<t_{n-p,\frac\alpha2}\right)=\prob\bigg(\hat\beta_i-t_{n-p,\frac{\alpha}2}\sigma_{\hat\beta_i}<\beta_i<\hat\beta_i+t_{n-p,\frac{\alpha}2}\sigma_{\hat\beta_i}\bigg)=1-\alpha$$

\proposition{Hypothesis Testing on $\beta_i$}
Suppose we want to test $H_0:\beta_i=\beta_{i0}$ against $H_1:\beta_i\neq\beta_{i0}$.\\
We use test statistic
$$T=\frac{\hat\beta_i-\beta_{i0}}{\hat\sigma_{\hat\beta_i}}$$
under $H_0$ $T\sim t_{n-p}$ where $n$ is the number of observations \& $p$ the number of parameters.\\
Thus we can assess the test using $p=\prob(|T|\geq|t_{obs}|)$.\\

\proposition{Testing Multiple Variables in a Model}
This can be expressed as the test of $H_0:\textbf{C}\pmb\beta=\textbf{d}$ against $H_1:\textbf{C}\pmb\beta\neq\textbf{d}$ where $\textbf{C}\in\reals^{q\times p}$ \& $\textbf{d}\in\reals^q$ with $q<p$.\\
Under $H_0$ we have $(\textbf{C}\hat{\pmb\beta}-\textbf{d})\sim\text{Normal}(\pmb0,\textbf{C}\Sigma_{\hat{\pmb\beta}}\textbf{C}^T)$.\\
We can produce a \textit{Cholesky Decomposition} $\textbf{L}^T\textbf{L}=\textbf{C}\Sigma_{\hat\beta}\textbf{C}^T$.\\
Thus
\[\begin{array}{rrcl}
&\textbf{L}^{-T}(\textbf{C}\hat{\pmb\beta}-\textbf{d})&\sim&\text{Normal}(0,I)\\
\implies&(\textbf{C}\hat{\pmb\beta}-\textbf{d})^T(\textbf{C}\Sigma_{\hat{\pmb\beta}}\textbf{C}^T)^{-1}(\textbf{C}\hat{\pmb\beta}-\textbf{d})&=&(\textbf{C}\hat{\pmb\beta}-\textbf{d})^T\textbf{L}^{-1}\textbf{L}^{-T}(\textbf{C}\hat{\pmb\beta}-\textbf{d})\\
&&=&\|\textbf{L}^{-T}(\textbf{C}\hat{\pmb\beta}-\textbf{d})\|^2\\
&&\sim&\displaystyle\sum_{i=1}^q\text{Normal}(0,1)^2\\
&&\sim&\chi^2_q
\end{array}\]
Setting $\hat\Sigma_{\hat{\pmb\beta}}:=\frac{\hat\sigma^2}{\sigma^2}\Sigma_{\hat{\pmb\beta}}$ we can produce a test statistic
$$F:=\frac1q(\textbf{C}\hat{\pmb\beta}-\textbf{d})^T(\textbf{C}\Sigma_{\hat{\pmb\beta}}\textbf{C}^T)^{-1}(\textbf{C}\hat{\pmb\beta}-\textbf{d})$$
Which has the distribution
\[\begin{array}{rcl}
F&=&\frac1q\|\textbf{L}^{-T}(\textbf{C}\hat{\pmb\beta}-\textbf{d})\|^2\\
&=&\dfrac{\sigma^2}{q\hat\sigma^2}\|\textbf{L}^{-T}(\textbf{C}\hat{\pmb\beta}-\textbf{d})\|^2\\
&=&\dfrac{\frac1q\|\textbf{L}^{-T}(\textbf{C}\hat{\pmb\beta}-\textbf{d})\|^2}{\hat\sigma^2/\sigma^2}\\
&=&\dfrac{\frac1q\|\textbf{L}^{-T}(\textbf{C}\hat{\pmb\beta}-\textbf{d})\|^2}{\frac1\sigma^2\frac1{n-p}\|\textbf{r}\|^2}\\
&\sim&\dfrac{\frac1q\chi^2_q}{\frac1{n-p}\chi^2_{n-p}}\\
&\sim&F_{q,n-p}
\end{array}\]

\proposition{$F=\dfrac{\frac1q(RSS_0-RSS_q)}{\frac1{n-p}RSS_1}$}
Where $RSS_0$ is the residual sum of squares when $H_0$ is true and $RSS_1$ is the residual sum of squares when $H_1$ is true.\\

\proposition{Testing whether a Factor Variable belongs in a Model}
\textit{Factor Variables} have multiple parameters associated to them in a model and thus to test whether the \textit{Factor Variable} should be in the model requires testing whether all of these parameters should equal 0.\\
This can be tested using the results in \textbf{Propostion 2.12} with $\textbf{d}=\pmb0$ and $\textbf{C}$ is the rows of the $I_p$ which indicate the parameters we wish to test.\\
In this case $q$ is the number of parmeters we wish to test. 

\subsection{Bayesian Approach}

\proposition{Prior Distributions}
We need to define \textit{Prior Distributions} for $\pmb\beta$ and $\sigma^2$.
$$\pmb\beta\sim\text{Normal}(\pmb\beta_0,\pmb\phi^{-1})\quad\text{and}\quad\frac1{\sigma^2}=:\tau\sim\Gamma(a,b)$$
where $\pmb\beta_0,\pmb\phi,a,b$ are given by us.\\
\nb In order for results to be tractable we use conjugate priors. $\tau:=\frac1{\sigma^2}$ is called \textit{Precision}.\\

\proposition{Resulting Distribution}
\[\begin{array}{rcl}
f(\textbf{y},\pmb\beta,\tau)&\propto&\displaystyle\frac{\tau^{\alpha-1+\frac{n}2}e^{-\frac\tau2\|\textbf{y}-\X\pmb\beta\|^2}}{e^{\frac12(\pmb\beta-\pmb\beta_0)^T\pmb\phi(\pmb\beta-\pmb\beta_0)}e^{-b\tau}}\\
f(\tau|\pmb\beta,\textbf{y})&\propto&\dfrac{\tau^{\alpha-1\frac{n}2}}{e^{\frac\tau2(b+\|\textbf{y}-\X\pmb\beta\|^2}}\\
&\sim&\Gamma\left(\frac{n}2+a,b+\frac12\|\textbf{y}-\X\pmb\beta\|^2\right)\\
f(\pmb\beta|\tau,\textbf{y})&\propto&\text{exp}\bigg\{-\frac12(\pmb\beta^T\X^T\X\pmb\beta\tau-2\pmb\beta\X^T\textbf{y}\tau+\pmb\beta^T\pmb\phi\pmb\beta-2\pmb\beta^T\pmb\phi\pmb\beta_0)\bigg\}\\
&\sim&\text{Normal}\left((\X^T\X\tau+\pmb\phi)^{-1}(\tau\X^T\textbf{y}+\pmb\phi\pmb\beta_0),\ (\X^T\X\tau+\pmb\phi)^{-1}\right)
\end{array}\]
If sample size tends to infinity or the prior precision matrix tends to $\pmb0$, then $$f(\pmb\beta|\tau,\textbf{y})\overset\sim\to\text{Normal}(\hat{\pmb\beta},(\X^T\X)^{-1}\sigma^2)$$
This is the same as in the \textit{Frequentist Approach}, so the same results acan be used for intervals \& hypothesis testing.\\
\nb As sample size tends to infinity $\X^T\X\tau$ dominates $\pmb\phi$.\\

\propositionn{Find Posterion, $f(\pmb\beta,\tau|\textbf{y})$}1
\begin{itemize}
	\item Iteratively find the posterior modes of $\pmb\beta$ (given the estimated mode of $\tau$) and the posterior mode of $\tau$ (given the estimated modes of $\pmb\beta$), until convergence.\\
	Then plug this into $f(\pmb\beta|\tau,\textbf{y})$.
	\item[Empirical Bayes] Integrate $\pmb\beta$ outo of $f(\tau|\pmb\beta,\textbf{y})$ to obtain $f(\tau|\textbf{y})$.\\
	Maximise $f(\tau|\textbf{y})$ to find $\hat\tau$.\\
	Then plug this into $f(\pmb\beta|\tau,\textbf{y})$.
	\item[Gibbs Sampling] Alternate simulation of $\pmb\beta$ from $f(\pmb\beta|\tau,\textbf{y})$ (give simulated $\tau$) with simulation of $\tau$ from $f(\tau|\pmb\beta,\textbf{y})$ (given simulated $\pmb\beta$)
\end{itemize}

\section{Maximum Likelihood Estimation}

\subsection{Frequentist}

\proposition{Frequentist Approach to Linear Models}
Parameters, $\pmb\beta$, are treataed as fixed states of nature and all uncertainty occurs in our estimation of these parameters.\\

\definition{Likelihood}
Let $\textbf{y}$ a set of $n$ observations from $f(\cdot;\pmb\theta)$.\\
\textit{Likelihood} measures the probability of observing specified outcomes, given a possible set of parameter values.
$$L_n(\pmb\theta;\textbf{y}):=f_n(\textbf{y};\pmb\theta)=\prod_{i=1}^nf(y_i;\pmb\theta)$$
Often we use the \textit{Log-Likelihood} function as it turns products into summations and exponents into parameter.
$$\ell_n(\pmb\theta;\textbf{y}):=\ln L(\pmb\theta;\textbf{y})=\sum_{i=1}^nf(y_i;\pmb\theta)$$
\nb $\argmax_\theta L(\theta)\equiv\argmax_\theta \ell(\theta)$.\\

\definition{Maximum Likelihood Estimate}
Let $\textbf{y}$ a set of $n$ observations from $f(\cdot;\pmb\theta)$.\\
The \textit{Maximum Likelihood Estimate},$\hat{\pmb\theta}_\text{MLE}$, for a set of parameter, $\pmb\theta$ is the set of parameter values which maximise the \textit{Likelihood Function}.
$$\hat{\pmb\theta}_\text{MLE}=\argmax_\theta L(\pmb\theta;\textbf{y})=\argmax_\theta \ell(\pmb\theta;\textbf{y})$$

\proposition{Finding Maximum Likelihood Estimate}
Let $\textbf{y}$ be a set $n$ of observations from the model $f(\textbf{X};\pmb\theta)$ with $|\pmb\theta|=m$.\\
To find the \textit{Maximum Likelihood Estimate} of $\pmb\beta$
\begin{enumerate}
	\item Define the \textit{Log-Likelihood Function} $\ell(\pmb\theta;\textbf{y})$.
	\item Find the gradient of $\ell(\pmb\theta;\textbf{y})$ wrt $\pmb\theta$
	$$\nabla\ell(\pmb\theta;\textbf{y}):=\begin{pmatrix}\frac\partial{\partial\theta_1}\ell(\pmb\theta;\textbf{y})&\dots&\frac\partial{\partial\theta_m}\ell(\pmb\theta;\textbf{y})\end{pmatrix}$$
	\item Equate the gradient to the zero-vector and solve for $\pmb\theta$ to find extrema of $\ell$
	$$\nabla\ell(\pmb\theta;\textbf{y})=\pmb0$$
	\item Calculate the \textit{Hessian} of $\ell(\pmb\theta;\textbf{x})$
	$$\nabla^2\ell(\pmb\beta;\textbf{y})=\begin{pmatrix}\frac{\partial}{\partial\theta_1^2}\ell(\pmb\theta;\textbf{y})&\dots&\frac{\partial}{\partial\theta_1\partial\theta_m}\ell(\pmb\theta;\textbf{y})\\\vdots&\ddots&\vdots\\\frac{\partial}{\partial\theta_m\partial\theta_1}\ell(\pmb\theta;\textbf{y})&\dots&\frac{\partial}{\partial\theta_m^2}\ell(\pmb\theta;\textbf{y})\end{pmatrix}$$
	\item Test each extremum, $\hat{\pmb\theta}$ to see if any are maxima
	\begin{center}
	If $\text{det}(H(\hat{\pmb\theta}))>0$ and $\frac\partial{\partial\theta_1^2}\ell(\hat{\pmb\theta};\textbf{y})<0$ then $\hat{\pmb\theta}$ is a local maximum.\\
	\ie If $H(\hat{\pmb\theta})$ is negative definite.
	\end{center}
\end{enumerate}

\remark{It is rare to find explicit expressions for MLEs. Instead we use \text{numerical optimisation}}

\subsection{Performance}

\remark{Here we look at properties of an MLE when we have a large sample size}

\definition{Fisher Information Matrix}
Let $\ell(\cdot):=\ln f(\textbf{y};\pmb\theta))\ \&\ \pmb\theta^*$ be the true parameter values.\\
\textit{Fisher Information} describes how much information the $X$ carries about the parameters, $\pmb\theta$.\\
The \textit{Fisher Information Matrix} is defined as
$$\mathcal{I}:=\expect\left(\dfrac{\partial\ell}{\partial\pmb\theta}\bigg|_{\pmb\theta=\pmb\theta^*}\dfrac{\partial\ell}{\partial\pmb\theta^T}\bigg|_{\pmb\theta=\pmb\theta^*}\right)$$

\proposition{Interpretting Fisher Information Matrix}
If $\mathcal{I}$ carries lots of information if it has large magnitude eigenvalues \& less information if they have small magnitude.\\
\nb See \textbf{Proposition 3.5} for a different formulation of \textit{Fisher Information Matrix}.\\

\remark{Properties of Expected Log-Likelihood}
Here are some properties of the \textit{Expected Log-Likelihood} as \textit{Large Sample Theory} of \textit{MLEs} relies on them.\\

\theorem{Expect a turning point in Log-Likelihood at the true parameter values}
Let $\ell(\cdot):=\ln f(\textbf{y};\pmb\theta))\ \&\ \pmb\theta^*$ be the true parameter values.
$$\expect\left(\dfrac{\partial\ell}{\partial\pmb\theta}\bigg|_{\pmb\theta=\pmb\theta^*}\right)=\pmb0$$
\textit{Proof}
$$\expect\left(\dfrac\partial{\partial\pmb\theta}\ln f(\textbf{y};\pmb\theta)\right)=\int\dfrac1{f(\textbf{y};\pmb\theta)}\dfrac{\partial f}{\partial\pmb\theta}f(\textbf{y};\pmb\theta)d\textbf{y}=\int\dfrac{\partial f}{\partial\pmb\theta}d\textbf{y}=\dfrac\partial{\partial\pmb\theta}\int f(\textbf{y};\pmb\theta)d\textbf{y}=\dfrac{\partial\pmb1}{\partial\pmb\theta}=\pmb0$$
\proved

\theorem{Covariance as Expectation}
Let $\ell(\cdot:=\ln f(\textbf{y};\pmb\theta))\ \&\ \pmb\theta^*$ be the true parameter values.
$$\cov\left(\dfrac{\partial\ell}{\partial\pmb\theta}\bigg|_{\pmb\theta=\pmb\theta^*}\right)=\expect\left(\dfrac{\partial\ell}{\partial\pmb\theta}\bigg|_{\pmb\theta=\pmb\theta^*}\dfrac{\partial\ell}{\partial\pmb\theta^T}\bigg|_{\pmb\theta=\pmb\theta^*}\right)$$
\textit{Proof}\\
Follows from \textbf{Theorem 3.1} and the defiinition of a covariance matrix, noting that $\frac{\partial\ell}{\partial\pmb\theta}$ is a column vector and $\frac{\partial\ell}{\partial\pmb\theta^T}$ is a row vector.\\

\proposition{Fisher Information Matrix as negative expectation}
Let $\ell(\cdot):=\ln f(\textbf{y};\pmb\theta))\ \&\ \pmb\theta^*$ be the true parameter values.
$$\mathcal{I}:=\expect\left(\dfrac{\partial\ell}{\partial\pmb\theta}\bigg|_{\pmb\theta=\pmb\theta^*}\dfrac{\partial\ell}{\partial\pmb\theta^T}\bigg|_{\pmb\theta=\pmb\theta^*}\right)\equiv-\expect\left(\dfrac{\partial^2\ell}{\partial\pmb\theta\partial\pmb\theta^T}\bigg|_{\pmb\theta=\pmb\theta^*}\right)$$
\textit{Proof}
\[\begin{array}{rrcl}
&\displaystyle\int\dfrac{\partial\ell}{\partial\pmb\theta}f(\textbf{y};\pmb\theta)d\textbf{y}&=&\pmb0\\
\implies&\displaystyle\int\dfrac{\partial^2\ell}{\partial\pmb\theta\partial\pmb\theta^T}f(\textbf{y};\pmb\theta)+\dfrac{\partial\ell}{\partial\pmb\theta}\dfrac{\partial f}{\partial\pmb\theta^T}d\textbf{y}&=&\pmb0\\
\text{but}&\dfrac{\partial\ell}{\partial\pmb\theta^T}&=&\dfrac1f\dfrac{\partial f}{\partial\pmb\theta^T}\\
\implies&\displaystyle\int\dfrac{\partial^2\ell}{\partial\pmb\theta\partial\pmb\theta^T}f(\textbf{y};\pmb\theta)d\textbf{y}&=&-\displaystyle\int\dfrac{\partial\ell}{\partial\pmb\theta}\dfrac{\partial\ell}{\partial\pmb\theta^T}f(\textbf{y};\pmb\theta)d\textbf{y}
\end{array}\]
\proved
\nb $\ell(\theta)=\ln f(\theta)\implies\frac{\partial}{\partial\theta}\ell(\theta)=\frac{\frac\partial{\partial\theta}f(\theta)}{f(\theta)}$.\\

\proposition{Expected Log-Likelihood has Global Maximum at True Parameter Value}
Let $\ell(\cdot):=\ln f(\textbf{y};\pmb\theta))\ \&\ \pmb\theta^*$ be the true parameter values.
$$\forall\ \pmb\theta\in\pmb\Theta,\ \expect[\ell(\pmb\theta)]\leq\expect[\ell(\pmb\theta^*)]$$
\textit{Proof}
Since $\ln$ is concave we can use \textit{Jensen's Inequality}
$$\expect\left[\ln\left(\dfrac{f(\textbf{y};\pmb\theta)}{f(\textbf{y};\pmb\theta^*)}\right)\right]\leq\ln\left[\expect\left(\dfrac{f(\textbf{y};\pmb\theta)}{f(\textbf{y};\pmb\theta^*)}\right)\right]=\ln\int\dfrac{f(\textbf{y};\pmb\theta)}{f(\textbf{y};\pmb\theta^*)}f(\textbf{y};\pmb\theta^*)d\textbf{y}=\ln\int f(\textbf{y};\pmb\theta)d\textbf{y}=\ln1=0$$
\proved

\theorem{Cram\'er-Rao Lower Bound}
Let $\textbf{y}\sim f(\cdot;\pmb\theta)$ and $\mathcal{I}$ be the \textit{Fisher Information Matrix}.\\
The \textit{Cram\'er-Rao Lower Bound} states that
\begin{center}$\mathcal{I}^{-1}$ is a lower bound of the variance matrix of any unbiased estimator $\tilde{\pmb\theta}$ \end{center}
In the sense that $\cov(\pmb\theta)-\mathcal{I}^{-1}$ is positive semi-definite.\\
\nb Look at proof.\\

\proposition{Consistency of MLE}
\textit{Maximum Likelihood Estimators} \underline{are} usually \textit{Consistent}.\\
This is because $\frac1n\ell(\pmb\theta)\overset{n\to\infty}{\longrightarrow}\frac{1}{n}\expect(\ell(\pmb\theta))\to\pmb\theta^*$ by \textbf{Proposition 3.5} (and WLLN if log can be broken down into summations).\\
\nb \textit{Consistency} likeliy fails when the number of parameters increases as sample sie increases.\\

\proposition{MLE Distribution for Large Sample Size}
Let $\hat{\pmb\theta}$ be an MLE for a set of parameters, with true value $\pmb\theta^*$.\\
As the same size tends to infty
$$\hat{\pmb\theta}\sim\text{Normal}(\pmb\theta^*,\mathcal{I}^{-1})$$
This means that in regular situtations, for large sample sizes, MLEs are unbiased and achieve the \textit{Cram\'er Rao Lower Bound}.\\

\proof{Proposition 3.7}
If the log-likelihood is based on independent observations then $\ell(\pmb\theta)=\sum_i\ell_i(\pmb\theta)$.\\
$\implies\frac{\partial\ell}{\partial\pmb\theta}=\sum_i\frac{\partial\ell_i}{\partial\pmb\theta}$.
Thus the central limit theorem applies and we get the result.\\
If the log-likelihood is \underline{not} base don independent observations then $\frac{\partial\ell}{\partial\pmb\theta}$ usually has a limiting normal distribution so the result holds anyways.

\subsection{Intervals}



\subsection{Numerical Optimisation}

\definition{Numerical Optimisation}
\textit{Numerical Optimisation} is the process of finding the set of parameters which maximise a function by numerically evaluating the function with multiple different values. This is used when we cannot take derivatives of a function for whatever reason.\\
\nb A lot of techniques focus on finding the \underline{minimum} so we use the negative of the \textit{Objective Function} in order to find the maximum instead.\\

\definition{Objective Function}
The \textit{Objective Function} is the function we wish to optimise in \textit{Numerical Optimisation}.\\
\nb In \textit{Statistical Inference} this is the \textit{Probability Density/Mass Function}.\\

\proposition{Assumptions about Objective Function}
In order to make problems easier we assume that the \textit{Objective Function} is:
\begin{enumerate}
	\item Sufficiently smooth;
	\item Bounded below; and,
	\item The parameter elements are unrestricted real values.\\
	If we want to put restrictions on $\pmb\theta$ we need to be able to implement them as $\pmb\theta=\textbf{r}(\pmb\theta_r)$ where $\textbf{r}(\cdot)$ is a known function \& $\pmb\theta_r$ is the unrestricted parameter set.
\end{enumerate}
\nb We require $f$ to be convex for \textit{Newton Methods} to work but that is an assumption too far. If it is not then we may only find a local minimum, not global.\\

\proposition{Requirements for Newton's Method}
In order to guarantee that \textit{Newton's Method} converges to the MLE, we need to ensure the followin
\begin{enumerate}
	\item The approximating quadratic actually has a maximum (not a minimum, inflection point etc.).\\
	If it has a minimum then we use its negative value instead.
	\item The proposed change in parameter values actually increases the \textit{Log-Likelihood} itself.\\
	If not we move the parameter back towards the previous parameter guess until the \textit{Log-Likelihood} increases.
\end{enumerate}

\definition{Newton's Method}
Let $f(\pmb\theta)$ be the function we wish to optimise (this would be the \textit{Likelihood Function}).\\
\textit{Newton's Method} is a method for \textit{Numerical Optimisation}.\\
The idea is to iteratively use a truncated \textit{Taylor Expansion} (to the second degree) of function $f(\pmb\theta)$ and to find the minimum of this approximation at each step.
\begin{enumerate}
	\item Make an initial input guess $\pmb\theta^{[0]}$. Set $k=0$.
	\item Evaluate the function \& its first two derivatives
	$$f(\pmb\theta^{[k]}),\ \nabla f(\pmb\theta^{[k]}),\ \nabla^2f(\pmb\theta^{[k]})$$
	\item \textbf{If} $\nabla f(\pmb\theta^{[k]})=\pmb0$ \textbf{and} $\nabla^2f(\pmb\theta^{[k]})$ is positive semi-definite\textbf{:}
	\begin{itemize}
		\item $\pmb\theta^{[k]}$ is a minimum. TERMINATE
	\end{itemize}
	\item \textbf{If} $\nabla^2f(\pmb\theta^{[k]})$ is \textit{positive-definite}\textbf{:} Set $\textbf{H}=\nabla^2f(\pmb\theta^{[k]})$.\\
	\textbf{Else:} Set $\textbf{H}=U\tilde{\Lambda}U^T$ where we have decomposed $\nabla^2f(\pmb\theta^{[k]})=U\Lambda U^T$ with $\Lambda$ being the diagonal matrix of eigenvalues \& $\tilde{\Lambda}_{ij}=|\Lambda_{ij}|$ (all values positive).\\
	\nb This step is to ensure that $\textbf{H}$ is \textit{postive definite}.
	\item Solve $\Delta:=-\dfrac{\nabla f(\pmb\theta^{[k]})}{\textbf{H}}$ where $\Delta$ is the search direction.
	\item \textbf{If not} $f(\pmb\theta^{[k]}+\Delta)<f(\pmb\theta^{[k]})$\textbf{:} repeatedly have $\Delta$ until condition is met.\\
	\nb Implementation of \textit{Step Length Control}.
	\item Set $\pmb\theta^{[k+1]}=\pmb\theta^{[k]}+\Delta$. Set $k+=1$
	\item Repeat ii)-vii) until we get a termination in stage iii).
\end{enumerate}
\nb In practice we terminate in iii) if $\|\nabla f(\pmb\theta^{[k]})\|<|\nabla f(\pmb\theta^{[k]})|\epsilon_r+\epsilon_a$, for some small $\epsilon_a,epsilon_r$ which we set.\\

\remark{\text{The first derivative tells us the direction, the second derivative suggests the step length}}

\remark{$f(\pmb\theta^{[k]})$ is only evaluated to ensure that step is an improvement.}
If $f(\cdot)$ is not avaiable (but $\nabla f\ \&\ \nabla^2 f$ are) we have a few options
\begin{itemize}
	\item Show that $f$ is non-increasing in the direction of the step, $\Delta$, at $\pmb\theta+\Delta$ (\ie $\nabla f(\pmb\theta+\Delta)^T\Delta\leq0$).
	\item[Or] Replace $-\nabla^2 \ell(\pmb\theta)$ with $-\expect[\nabla^2\ell(\pmb\theta)]$ (Known as the \textit{Fisher Scoring Matrix}).
\end{itemize}
\nb This only affects step vi) of \textbf{Definition 3.5}.\\

\remarkk{Other Numerical Optimisation Techniques}
\begin{itemize}
	\item[Quasi-Newton] \textit{Quasi-Newton Methods} are \textit{Newton type Methods} in which an approximation is made for the \textit{Hessian Matrix} ($\nabla^2 f(\cdot)$), or its inverse, by building it from the first derivative information computed at each trial.\\
	\nb In R this is done with $\mathtt{optim(\dots,method='BFGS')}$.
	\item[Steepest Descent] Truncating the \textit{Taylor Expansion} allows us to establish the direction to step but not length. Thus we need to implement step length control methods.
\end{itemize}
\nb There plenty of methods not covered here.

\section{Estimating Posterior Distribution}
%TODO Slice sampling 10.3.2
\proposition{Difficulty Computing Bayes' Theorem} %TODO HERE
The \textit{Likelihood} \& \textit{Prior Distributions} are computable in \textit{Bayes' Theorem}, but the \textit{Evidence Distribution}, $p(\textbf{y})$ is not. Thus the difficult in calculating the \textit{Posterior Distribution} lies in trying to calculate the \textit{Evidence Distribution} for the observed data.\\

\remark{Likelihood is the probability of observing the data that we observed}

\definition{Markov Chains} % 9.1
\definition{Detailed Balanced Condition} %9.2
\definition{MCMC Methods} % 9

\subsection{Metropolis Hastings}
\proposition{Metropolis Hastings Method} % 9.3-9.6
\remark{Convergence} % 9.10
\remark{Interval Estimation} % 9.11

\subsection{Gibbs Sampling}
\proposition{Gibbs Sampling} % 9.7-9.9
\remark{Convergence} % 9.10
\remark{Interval Estimation} % 9.11

\subsection{Estimating Posterior Point-Value}

\definition{Posterior Mode} %11

\section{Bayesian Inference}

\definition{Marginal Likelihood} % 11.1.1
\definition{Bayes Factor} % 11.1.1 & 11.1.2
\definition{Bayesian Information Criterion} %11.1.3
\definition{Deviance Information Criterion} %11.1.4

\newpage\setcounter{section}{-1}
\section{Appendix}

\subsection{Definitions}

\definition{Parametric Models}
\textit{Parameteric Models} are \textit{Statistical Models} whose only unkowns are parameters.\\

\definition{Semi-Parametric Models}
\textit{Parameteric Models} are \textit{Statistical Models} which contain unknown parameters \underline{and} unknown functions.\\

\definition{Non-Parametric Models}
\textit{Non-Parametric Models} make \textit{few} prior assumptions about how data was generated and instead depend mainly on the observed data.\\
We \underline{cannot} simulate data from \textit{Non-Parameteric Models}.\\

\definition{Orthongonal Matrix}
A matrix $\X$ is \textit{Orthogonal} if
$$\X^T\X=\X\X^T=I\implies\X^T=\X^{-1}$$
\textit{Orthogonal Matrices} rotate \& reflect vectors without changing their magnitude.\\
\nb $\X^T$ is \textit{Orthogonal}.\\

\definition{Full Rank Matrix}
Let $\X\in\reals^{m\times n}$.\\
If $m>n$ then $\X$ has \textit{Full Rank} iff all its \underline{columns} are linearly independent.\\
If $n>m$ then $\X$ has \textit{Full Rank} iff all its \underline{rows} are linearly independent.\\
\nb In statistics the number of $m>n$ always as we should have more observations than fields.\\

\definition{Upper Triangle Matrix}
A matrix $X$ is an \textit{Upper Triangle Matrix} if $X_{i,j}=0$ for $i>j$.\\

\definition{Unbiased Estimator}
An \textit{Estimator} of a parameter, $\hat\theta$, is unbiased if its expected value is the true value of the parameter for all possible parameter values
$$\expect(\hat\theta;\theta=\theta^*)=\theta^*$$

\definition{Conjugacy}
\definition{Fisher Information}
\definition{Correlation}
\definition{Covariance}
\definition{Expected Value}
\definition{Variance}

\definition{Positive Definite Matrix}
A matrix is \textit{Positive Definite} if it is symmetric and all its eigenvalues are \underline{positive}.\\

\definition{Positive Semi-Definite Matrix}
A matrix is \textit{Postitive Semi-Definite} if all its eigenvalues are \underline{non-negative}.

\definition{Taylor's Theorem}
\definition{Casual Inference}

\definition{Consistent Estimator}
A \textit{Parameter Estimator}, $\hat{\pmb\theta}_n$, is \textit{Consistent} if its value tends to the true parameter value, $\pmb\theta^*$, as sample size tends to infinity
$$\hat{\pmb\theta}_n\overset{n\to\infty}{\longrightarrow}\pmb\theta^*$$

\subsection{Theorems}

\theorem{Bayes' Theorem}
Suppose $X\sim f(\cdot;\Theta)$. Then
$$\underbrace{\prob(\Theta|X)}_\text{Posterior}=\frac{\overbrace{\prob(X|\Theta)}^\text{Likelihood}\overbrace{\prob(\Theta)}^\text{Prior}}{\underbrace{\prob(X)}_\text{Evidence}}$$

\theorem{Euclidean Distance Identities}
$$\|\x\|^2=\x^T\x=\sum_{i=1}^nx_i^2$$

\theorem{}
If $\X$ and $\textbf{Y}$ are independent. Then
$$\X\textbf{Y}\simeq\pmb0$$
%TODO find the formal version of this

\theorem{Jensen's Inequality}
For any random variable $X$ and concave function $f$
$$f[\expect(X)]\geq\expect[f(X)]$$

\end{document}
